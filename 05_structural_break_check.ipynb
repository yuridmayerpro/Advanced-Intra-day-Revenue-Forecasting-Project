{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Databricks notebook source\n",
        "\n",
        "\n",
        "This notebook performs the Chow test to verify if a structural break occurred in the NET_VALUE time series.\n",
        "\n",
        "A structural break can affect the mean, variance, seasonality, or dependence between series observations, causing predictive model degradation.\n",
        "\n",
        "Model degradation due to a structural break occurs because the model is not adjusted to the new detected behavior, requiring retraining with new data to adapt the model to the new pattern."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n",
        "## 1. Libraries"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "!pip install holidays==0.45"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Statistical analysis\n",
        "from scipy import stats\n",
        "import statsmodels.api as sm\n",
        "\n",
        "# Utilities\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from datetime import timedelta\n",
        "import holidays\n",
        "from sklearn.preprocessing import PowerTransformer\n",
        "from statsmodels.tsa.seasonal import seasonal_decompose\n",
        "\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n",
        "## 2. Data Import and Preparation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Sales Series\n",
        "str_select_sales = '''\n",
        "                        SELECT *\n",
        "                        FROM analytics.refined_sales_orders_agg\n",
        "                        '''\n",
        "\n",
        "df_series_sales = spark.sql(str_select_sales).toPandas()\n",
        "df_series_sales.sort_values('SYSTEM_TIMESTAMP', inplace=True)\n",
        "df_series_sales.set_index('SYSTEM_TIMESTAMP', inplace=True)\n",
        "\n",
        "# Converts granularity to Daily\n",
        "df_series_sales_resampled = df_series_sales.resample('D').sum()\n",
        "\n",
        "# Outlier removal\n",
        "# Decomposing the series for residual outlier verification\n",
        "result = seasonal_decompose(df_series_sales_resampled['NET_VALUE'], model='additive', period=8)\n",
        "trend = result.trend\n",
        "seasonal = result.seasonal\n",
        "residual = result.resid\n",
        "\n",
        "# Calculates residual z-scores\n",
        "z_scores = stats.zscore(residual.dropna())\n",
        "\n",
        "# Gets outlier indices\n",
        "zscore_threshold = 3 # Since the series has seasonality and high amplitude, a lower threshold would cut natural peaks\n",
        "outliers = np.abs(z_scores) > zscore_threshold\n",
        "outliers_index = residual.dropna().index[outliers]\n",
        "\n",
        "# Residual interpolation at outlier points\n",
        "residual_adjusted = residual.copy()\n",
        "residual_adjusted[outliers_index] = np.nan\n",
        "residual_adjusted = residual_adjusted.interpolate()\n",
        "residual_adjusted = residual_adjusted.fillna(method='bfill').fillna(method='ffill')\n",
        "\n",
        "# Reconstruction of adjusted time series\n",
        "adjusted_time_series = trend + seasonal + residual_adjusted\n",
        "df_series_sales_resampled['NET_VALUE_clean'] = adjusted_time_series\n",
        "df_series_sales_resampled['NET_VALUE_clean'] = df_series_sales_resampled['NET_VALUE_clean'].fillna(df_series_sales_resampled['NET_VALUE'])\n",
        "\n",
        "df_series_sales_resampled.info()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Function to calculate Easter date\n",
        "def calculate_easter(year):\n",
        "    \"Returns the Easter date for a given year.\"\n",
        "    a = year % 19\n",
        "    b = year // 100\n",
        "    c = year % 100\n",
        "    d = b // 4\n",
        "    e = b % 4\n",
        "    f = (b + 8) // 25\n",
        "    g = (b - f + 1) // 3\n",
        "    h = (19 * a + b - d - g + 15) % 30\n",
        "    i = c // 4\n",
        "    k = c % 4\n",
        "    l = (32 + 2 * e + 2 * i - h - k) % 7\n",
        "    m = (a + 11 * h + 22 * l) // 451\n",
        "    month = (h + l - 7 * m + 114) // 31\n",
        "    day = ((h + l - 7 * m + 114) % 31) + 1\n",
        "    return pd.Timestamp(year, month, day)\n",
        "\n",
        "# Function to calculate Carnival date\n",
        "def calculate_carnival(year):\n",
        "    \"Returns the Carnival date for a given year.\"\n",
        "    easter = calculate_easter(year)\n",
        "    return easter - timedelta(days=47)\n",
        "    \n",
        "# Function to determine custom events\n",
        "def custom_events(date):\n",
        "    month_day = date.strftime('%m-%d')\n",
        "    year = date.year\n",
        "\n",
        "    # Carnival\n",
        "    carnival = calculate_carnival(year)\n",
        "    # Carnival Eve\n",
        "    carnival_eve = carnival - timedelta(days=1)\n",
        "    # Mother's Day (second Sunday of May)\n",
        "    mothers_day = pd.Timestamp(year, 5, 1) + pd.DateOffset(weekday=6, weeks=1)\n",
        "    # Mother's Day Eve\n",
        "    mothers_day_eve = mothers_day - timedelta(days=1)\n",
        "    # Valentines Day (June 12th in Brazil)\n",
        "    valentines_day = pd.Timestamp(year, 6, 12)\n",
        "    # Black Friday (fourth Friday of November)\n",
        "    black_friday = pd.Timestamp(year, 11, 1) + pd.DateOffset(weekday=4, weeks=3)\n",
        "    \n",
        "    if date == carnaval:\n",
        "        return 'Carnival'\n",
        "    elif date == carnival_eve:\n",
        "        return 'Carnival Eve'\n",
        "    elif month_day == mothers_day.strftime('%m-%d'):\n",
        "        return 'Mothers Day'\n",
        "    elif month_day == mothers_day_eve.strftime('%m-%d'):\n",
        "        return 'Mothers Day Eve'\n",
        "    elif month_day == valentines_day.strftime('%m-%d'):\n",
        "        return 'Valentines Day'\n",
        "    elif month_day == black_friday.strftime('%m-%d'):\n",
        "        return 'Black Friday'\n",
        "    else:\n",
        "        return None\n",
        "\n",
        "# Function to include events in holiday column\n",
        "def include_events(row):\n",
        "    if pd.isna(row['event']):\n",
        "        return custom_events(row.name)\n",
        "    else:\n",
        "        return row['event']"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Including holiday\n",
        "brazil_holidays = holidays.Brazil()\n",
        "df_series_sales_resampled['event'] = df_series_sales_resampled.index.map(lambda x: brazil_holidays.get(x, None))\n",
        "\n",
        "# Adding custom holidays\n",
        "df_series_sales_resampled['event'] = df_series_sales_resampled.apply(include_events, axis=1)\n",
        "\n",
        "df_series_sales_resampled['day_of_week'] = df_series_sales_resampled.index.day_of_week #  Monday=0 and Sunday=6\n",
        "\n",
        "# Power transform\n",
        "scaler = PowerTransformer()\n",
        "df_series_sales_resampled['NET_VALUE_clean_scld'] = scaler.fit_transform(df_series_sales_resampled[['NET_VALUE_clean']])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "df_series_sales_resampled.info()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n",
        "## 3. Structural Break Verification"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "def chow_test(serie_before, serie_after):\n",
        "\n",
        "    '''\n",
        "    Calculates the Chow test for a time series, determining if there is a structural break \n",
        "    between two specified periods.\n",
        "\n",
        "    The Chow test is used to verify if there is a significant difference between coefficients \n",
        "    of two linear regressions in different time periods, suggesting a structural change in the time series.\n",
        "\n",
        "    Parameters:\n",
        "    -----------\n",
        "    serie_before : pandas.Series\n",
        "        Time series corresponding to the period before the possible structural break.\n",
        "    serie_after : pandas.Series\n",
        "        Time series corresponding to the period after the possible structural break.\n",
        "\n",
        "    Returns:\n",
        "    --------\n",
        "    chow : float\n",
        "        The Chow F-test value, which can be compared to an F distribution to determine the significance\n",
        "        of the structural break.\n",
        "\n",
        "    References:\n",
        "    ------------\n",
        "    - Chow, Gregory C. \"Tests of equality between sets of coefficients in two linear regressions.\" Econometrica: \n",
        "      Journal of the Econometric Society (1960): 591-605.\n",
        "    - Toyoda, Toshihisa. \"Use of the Chow test under heteroscedasticity.\" Econometrica: Journal of the Econometric \n",
        "      Society (1974): 601-608.\n",
        "    - https://en.wikipedia.org/wiki/Chow_test (Accessed on 06/19/2024)\n",
        "    '''\n",
        "\n",
        "\n",
        "    # Pooled regression\n",
        "    all_series = pd.concat([serie_before, serie_after])\n",
        "    X = all_series.shift()[1:]\n",
        "    y = all_series[1:]\n",
        "    result_pooled = sm.OLS(y, X).fit()\n",
        "    ssr_pooled = result_pooled.ssr\n",
        "\n",
        "    # Regression for each period\n",
        "    X_before = serie_before.shift()[1:]\n",
        "    y_before = serie_before[1:]\n",
        "\n",
        "    X_after = serie_after.shift()[1:]\n",
        "    y_after = serie_after[1:]\n",
        "\n",
        "    result_before = sm.OLS(y_before, X_before).fit()\n",
        "    result_after = sm.OLS(y_after, X_after).fit()\n",
        "\n",
        "    ssr_1 = result_before.ssr\n",
        "    ssr_2 = result_after.ssr\n",
        "\n",
        "    # F-test for coefficients\n",
        "    k = 2 # degrees of freedom: slope and intercept\n",
        "    N1 = len(X_before) # number of observations before break\n",
        "    N2 = len(X_after) # number of observations after break\n",
        "    chow = ((ssr_pooled - (ssr_1 + ssr_2)) / k) / ((ssr_1 + ssr_2) / (N1+N2-2*k))\n",
        "\n",
        "    return chow"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Creating time series to be analyzed\n",
        "series = df_series_sales_resampled[(df_series_sales_resampled['event'].isna()) & (df_series_sales_resampled['day_of_week'] < 5)][['NET_VALUE_clean_scld']]\n",
        "\n",
        "# Defining periods of interest\n",
        "months_to_analyze = 3\n",
        "current_end = series.index.max()  # Last 3 months\n",
        "current_start = current_end - pd.DateOffset(months=months_to_analyze)\n",
        "\n",
        "comparison_end = current_end - pd.DateOffset(years=1)  # Same period last year\n",
        "comparison_start = comparison_end - pd.DateOffset(months=months_to_analyze)\n",
        "\n",
        "# Separating series between periods to be checked\n",
        "serie_before = series[comparison_start:comparison_end]\n",
        "serie_after = series[current_start:current_end]\n",
        "\n",
        "# Applying Chow statistic\n",
        "f_chow = chow_test(serie_before, serie_after)\n",
        "\n",
        "# Calculating critical value\n",
        "k = 2 # degrees of freedom: slope and intercept\n",
        "N1 = len(serie_before[1:]) # number of observations before break\n",
        "N2 = len(serie_after[1:]) # number of observations after break\n",
        "critical_value = stats.f.ppf(q=0.99, dfn=k, dfd= N1+N2 -(2*k))\n",
        "\n",
        "# Checking results\n",
        "print(f'Chow Statistic: {round(f_chow, 3)}')\n",
        "print(f'Critical Value: {round(critical_value, 3)}')\n",
        "\n",
        "if f_chow > critical_value:\n",
        "    print('H0 rejected: Periods are structurally different')\n",
        "    print('Result: retrain')\n",
        "    result = 'retrain'\n",
        "else:\n",
        "    print('H0 accepted: Periods are structurally equal')\n",
        "    print('Result: keep')\n",
        "    result = 'keep'"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Returns whether retraining should happen. This output is used by the Synapse pipeline to trigger the retraining notebook if necessary.\n",
        "dbutils.notebook.exit(result)"
      ],
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 4
}