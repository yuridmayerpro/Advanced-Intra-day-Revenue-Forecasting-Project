{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Databricks notebook source\n",
        "\n",
        "## 1. Libraries"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "!pip install u8darts[all]==0.29.0\n",
        "!pip install mlflow==2.11.3\n",
        "!pip install holidays==0.45\n",
        "dbutils.library.restartPython()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Darts\n",
        "from darts import TimeSeries\n",
        "from darts.models.forecasting.lgbm import LightGBMModel\n",
        "from darts import TimeSeries\n",
        "from darts import metrics\n",
        "from darts.dataprocessing.transformers import Scaler\n",
        "\n",
        "# Analysis\n",
        "from statsmodels.tsa.seasonal import seasonal_decompose\n",
        "from statsmodels.stats.diagnostic import acorr_ljungbox\n",
        "from statsmodels.graphics.tsaplots import plot_acf, plot_pacf\n",
        "\n",
        "# Plotting\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import plotly.express as px\n",
        "\n",
        "# Utilities\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import datetime\n",
        "import mlflow\n",
        "import mlflow.sklearn\n",
        "from numpy import savetxt\n",
        "from datetime import timedelta\n",
        "import holidays\n",
        "from scipy import stats\n",
        "from sklearn.preprocessing import MinMaxScaler, PowerTransformer\n",
        "from sklearn.linear_model import LinearRegression\n",
        "from statsmodels.tsa.stattools import acf, pacf\n",
        "from dateutil.relativedelta import relativedelta\n",
        "from tqdm import tqdm\n",
        "import pickle\n",
        "\n",
        "# Notebook configuration\n",
        "%matplotlib inline\n",
        "%config InlineBackend.figure_format = 'retina'\n",
        "sns.set_theme(style=\"whitegrid\", palette=\"pastel\")\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n",
        "## 2. Utility Functions"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Function to calculate Easter date\n",
        "def calculate_easter(year):\n",
        "    \"Returns the Easter date for a given year.\"\n",
        "    a = year % 19\n",
        "    b = year // 100\n",
        "    c = year % 100\n",
        "    d = b // 4\n",
        "    e = b % 4\n",
        "    f = (b + 8) // 25\n",
        "    g = (b - f + 1) // 3\n",
        "    h = (19 * a + b - d - g + 15) % 30\n",
        "    i = c // 4\n",
        "    k = c % 4\n",
        "    l = (32 + 2 * e + 2 * i - h - k) % 7\n",
        "    m = (a + 11 * h + 22 * l) // 451\n",
        "    month = (h + l - 7 * m + 114) // 31\n",
        "    day = ((h + l - 7 * m + 114) % 31) + 1\n",
        "    return pd.Timestamp(year, month, day)\n",
        "\n",
        "\n",
        "# Function to calculate Carnival date\n",
        "def calculate_carnival(year):\n",
        "    \"Returns the Carnival date for a given year.\"\n",
        "    easter = calculate_easter(year)\n",
        "    return easter - timedelta(days=47)\n",
        "    \n",
        "\n",
        "# Function to determine custom events\n",
        "def custom_events(date):\n",
        "    month_day = date.strftime('%m-%d')\n",
        "    year = date.year\n",
        "\n",
        "    # Carnival\n",
        "    carnival = calculate_carnival(year)\n",
        "    # Carnival Eve\n",
        "    carnival_eve = carnival - timedelta(days=1)\n",
        "    # Mother's Day (second Sunday of May)\n",
        "    mothers_day = pd.Timestamp(year, 5, 1) + pd.DateOffset(weekday=6, weeks=1)\n",
        "    # Mother's Day Eve\n",
        "    mothers_day_eve = mothers_day - timedelta(days=1)\n",
        "    # Valentines Day (June 12th in Brazil)\n",
        "    valentines_day = pd.Timestamp(year, 6, 12)\n",
        "    # Black Friday (fourth Friday of November)\n",
        "    black_friday = pd.Timestamp(year, 11, 1) + pd.DateOffset(weekday=4, weeks=3)\n",
        "    \n",
        "    if date == carnival:\n",
        "        return 'Carnival'\n",
        "    elif date == carnival_eve:\n",
        "        return 'Carnival Eve'\n",
        "    elif month_day == mothers_day.strftime('%m-%d'):\n",
        "        return 'Mothers Day'\n",
        "    elif month_day == mothers_day_eve.strftime('%m-%d'):\n",
        "        return 'Mothers Day Eve'\n",
        "    elif month_day == valentines_day.strftime('%m-%d'):\n",
        "        return 'Valentines Day'\n",
        "    elif month_day == black_friday.strftime('%m-%d'):\n",
        "        return 'Black Friday'\n",
        "    else:\n",
        "        return None\n",
        "\n",
        "\n",
        "# Function to include events in holiday column\n",
        "def include_events(row):\n",
        "    if pd.isna(row['event']):\n",
        "        return custom_events(row.name)\n",
        "    else:\n",
        "        return row['event']\n",
        "    \n",
        "\n",
        "# Function to replace values\n",
        "def replace_event(row):\n",
        "    if row['event'] not in events_to_consider:\n",
        "        return 'Normal Day'\n",
        "    else:\n",
        "        return row['event']\n",
        "\n",
        "def replace_or_add(lst, index, new_value):\n",
        "    if index < len(lst):\n",
        "        # If index exists in list, replace item\n",
        "        lst[index] = new_value\n",
        "    else:\n",
        "        # If index does not exist, append new value to end of list\n",
        "        lst.append(new_value)\n",
        "\n",
        "\n",
        "# Function to remove outliers from metrics\n",
        "def remove_outliers_zscore(data_list, threshold=3):\n",
        "    # Remove null values\n",
        "    data_np = np.array([x for x in data_list if x is not np.nan])\n",
        "\n",
        "    if len(data_np) == 0:\n",
        "        return []\n",
        "\n",
        "    # Calculate z-scores and filter outliers\n",
        "    z_scores = np.abs((data_np - np.mean(data_np)) / np.std(data_np))\n",
        "    filtered_data = data_np[z_scores < threshold]\n",
        "\n",
        "    return filtered_data.tolist()\n",
        "\n",
        "    \n",
        "def prepare_input(df, \n",
        "                  target_col='NET_VALUE',\n",
        "                  numeric_features=None, \n",
        "                  use_event_flag=False, \n",
        "                  events_to_consider=None,\n",
        "                  scale_trgt=False,\n",
        "                  scale_cov=False,\n",
        "                  scale_method=None,\n",
        "                  dummy_day_of_week=False):\n",
        "    '''\n",
        "    Function to prepare model input data.\n",
        "\n",
        "    Parameters:\n",
        "    - df (pandas.DataFrame): DataFrame containing the data used.\n",
        "    - target_col (str): Name of the target series column.\n",
        "    - numeric_features (list): List with numerical feature columns to be used.\n",
        "    - use_event_flag (boolean): Whether to use the event flag.\n",
        "    - events_to_consider (list): List with event names to consider if used as variables.\n",
        "    - scale_trgt (boolean): Whether to scale the target variable.\n",
        "    - scale_cov (boolean): Whether to scale the covariates.\n",
        "    - scale_method (str): Name of the method to be used when scaling data (minmax, power). \n",
        "\n",
        "    Returns:\n",
        "    - list with the following items:\n",
        "        - series (TimeSeries (DataArray)): target series.\n",
        "        - series_cov (TimeSeries (DataArray)): covariate series.\n",
        "        - transformer_trgt (darts.dataprocessing.transformers.scaler.Scaler): target series scaler for scale reversal.\n",
        "    '''\n",
        "    return_list = []\n",
        "\n",
        "    # Generates main series\n",
        "    series = TimeSeries.from_dataframe(\n",
        "                                        df=df,\n",
        "                                        value_cols=target_col,\n",
        "                                        freq='D')\n",
        "    return_list.append(series)\n",
        "\n",
        "    # Initialize series_cov\n",
        "    series_cov = None\n",
        "\n",
        "    # Creates covariate series for numerical variables\n",
        "    if numeric_features:\n",
        "        series_cov =  TimeSeries.from_dataframe(\n",
        "                                                df=df,\n",
        "                                                value_cols=numeric_features,\n",
        "                                                freq='D')\n",
        "        replace_or_add(return_list, 1, series_cov)\n",
        "\n",
        "    # Creates series for event_flag variable\n",
        "    if use_event_flag:\n",
        "        series_event_flag = TimeSeries.from_dataframe(\n",
        "                                                        df=df,\n",
        "                                                        value_cols='event_flag',\n",
        "                                                        freq='D')\n",
        "        \n",
        "        series_cov = series_cov.stack(series_event_flag) if series_cov else series_event_flag\n",
        "        replace_or_add(return_list, 1, series_cov)\n",
        "\n",
        "    # Creates series with dummy of events to consider\n",
        "    if events_to_consider:\n",
        "        df['event'] = df.apply(replace_event, axis=1)\n",
        "        df_dummy = pd.get_dummies(df['event'], prefix='event') * 1\n",
        "        series_dummy = TimeSeries.from_dataframe(\n",
        "                                                    df=df_dummy,\n",
        "                                                    value_cols=df_dummy.columns.tolist(),\n",
        "                                                    freq='D')\n",
        "        \n",
        "        series_cov = series_cov.stack(series_dummy) if series_cov else series_dummy\n",
        "        replace_or_add(return_list, 1, series_cov)\n",
        "\n",
        "    # Creates series with day of week dummy\n",
        "    if dummy_day_of_week:\n",
        "        df_dummy_dayofweek = pd.get_dummies(df['day_of_week'], prefix='day_of_week') * 1\n",
        "        series_dummy_dayofweek = TimeSeries.from_dataframe(\n",
        "                                                            df=df_dummy_dayofweek,\n",
        "                                                            value_cols=df_dummy_dayofweek.columns.tolist(),\n",
        "                                                            freq='D')\n",
        "        series_cov = series_cov.stack(series_dummy_dayofweek) if series_cov else series_dummy_dayofweek\n",
        "        replace_or_add(return_list, 1, series_cov)\n",
        "\n",
        "    # Initialize transformers\n",
        "    transformer_trgt = None\n",
        "    transformer_cov = None\n",
        "\n",
        "    # Performs scaling if desired\n",
        "    if scale_trgt:\n",
        "        if scale_method == 'minmax':\n",
        "            scaler_trgt = MinMaxScaler()\n",
        "        else:\n",
        "            scaler_trgt = PowerTransformer()\n",
        "            \n",
        "        transformer_trgt = Scaler(scaler_trgt)\n",
        "        series = transformer_trgt.fit_transform(series)\n",
        "        replace_or_add(return_list, 0, series)\n",
        "        replace_or_add(return_list, 2, transformer_trgt)\n",
        "\n",
        "    if scale_cov and series_cov:\n",
        "        if scale_method == 'minmax':\n",
        "            scaler_cov = MinMaxScaler()\n",
        "        else:\n",
        "            scaler_cov = PowerTransformer()\n",
        "            \n",
        "        transformer_cov = Scaler(scaler_cov)\n",
        "        series_cov = transformer_cov.fit_transform(series_cov)\n",
        "        replace_or_add(return_list, 1, series_cov)\n",
        "\n",
        "\n",
        "    return return_list\n",
        "\n",
        "\n",
        "# Function to create the model\n",
        "def create_model(use_covariates, lags, output_chunk_length, random_seed, n_estimators, multi_models, lags_past_covariates, lags_future_covariates):\n",
        "    if use_covariates:\n",
        "        return LightGBMModel(\n",
        "            lags=lags,\n",
        "            output_chunk_length=output_chunk_length,\n",
        "            lags_past_covariates=lags_past_covariates,\n",
        "            lags_future_covariates=lags_future_covariates,\n",
        "            random_state=random_seed,\n",
        "            n_estimators=n_estimators,\n",
        "            multi_models=multi_models,\n",
        "            force_row_wise=False,\n",
        "            force_col_wise=False)\n",
        "    else:\n",
        "        return LightGBMModel(\n",
        "            lags=lags,\n",
        "            output_chunk_length=output_chunk_length,\n",
        "            random_state=random_seed,\n",
        "            n_estimators=n_estimators,\n",
        "            multi_models=multi_models,\n",
        "            force_row_wise=False,\n",
        "            force_col_wise=True)\n",
        "\n",
        "\n",
        "# Transform negative values to zero\n",
        "def replace_negatives_with_zero(x):\n",
        "    return np.maximum(x, 0)\n",
        "\n",
        "\n",
        "# Function to calculate WAPE\n",
        "def calculate_wape(y, yhat):\n",
        "    \"\"\"\n",
        "    Calculates Weighted Absolute Percentage Error (WAPE).\n",
        "    \n",
        "    Args:\n",
        "        y (list or numpy array): Actual values.\n",
        "        yhat (list or numpy array): Predicted values.\n",
        "    \n",
        "    Returns:\n",
        "        float: WAPE value.\n",
        "        \n",
        "    Description:\n",
        "    This function calculates the Weighted Absolute Percentage Error (WAPE).\n",
        "    \n",
        "    WAPE takes into account both absolute error and percentage error between actual and predicted values. \n",
        "    Furthermore, it performs error weighting based on actual values. In seasonal sales time series scenarios, \n",
        "    seasonal peak periods are assigned a higher weight, so errors in these periods have a more significant impact on this metric.\n",
        "    \n",
        "    Weighting is performed by assigning higher weights to larger actual values, since errors in high demand periods \n",
        "    can have a more relevant impact on planning decisions and financial results.\n",
        "    \n",
        "    In the code section where weight is defined as 'weight = actual', it is considered that the weight of each observation \n",
        "    is equal to the actual value of that observation itself. This means that when calculating WAPE, absolute percentage errors \n",
        "    are multiplied by the actual values themselves before being summed to calculate total weighted error.\n",
        "    \"\"\"\n",
        "    \n",
        "    # Initializes variables to store total WAPE and total weights\n",
        "    total_wape = 0\n",
        "    total_weight = 0\n",
        "    \n",
        "    try:\n",
        "        # Loop over actual (y) and predicted (yhat) values\n",
        "        for i in range(len(y)):\n",
        "            actual = y[i]  # Actual value\n",
        "            predicted = yhat[i]  # Predicted value\n",
        "            \n",
        "            # Calculates absolute error between actual and predicted value\n",
        "            absolute_error = abs(actual - predicted)\n",
        "            \n",
        "            # Calculates absolute percentage error\n",
        "            absolute_percentage_error = absolute_error / actual\n",
        "            \n",
        "            # Calculates weight as the actual value (used for weighting)\n",
        "            weight = actual\n",
        "            \n",
        "            # Updates total WAPE summing weighted percentage error\n",
        "            total_wape += absolute_percentage_error * weight\n",
        "            \n",
        "            # Updates total weights\n",
        "            total_weight += weight\n",
        "        \n",
        "        # Calculates final WAPE as weighted average of percentage errors\n",
        "        wape = total_wape / total_weight * 100\n",
        "    except:\n",
        "        # If an exception occurs (e.g., division by zero), sets WAPE as infinite\n",
        "        wape = np.inf\n",
        "        \n",
        "    return wape\n",
        "\n",
        "\n",
        "# Function to perform predictions and calculate metrics\n",
        "def predict_and_evaluate(model, serie_tgrt, serie_tgrt_test, series_cov, scale_trgt, transformer_trgt, target_col):\n",
        "    df_preds = pd.DataFrame(columns=['NET_VALUE_predicted_05', 'NET_VALUE_predicted', 'NET_VALUE_predicted_95', 'NET_VALUE_real'])\n",
        "    metrics_lists = {\n",
        "        'mape': [], 'smape': [], 'ope': [], 'r2': [], 'rmse': [], 'wape': []\n",
        "    }\n",
        "\n",
        "    unique_dates = serie_tgrt_test.time_index.strftime('%Y-%m-%d').unique()\n",
        "    min_time = serie_tgrt.time_index.min()\n",
        "\n",
        "    progress_bar_general = tqdm(unique_dates, desc='Processing')\n",
        "    for day in progress_bar_general:\n",
        "        start_time = pd.Timestamp(day)\n",
        "        start_real = pd.Timestamp(day) - pd.Timedelta(days=1)\n",
        "        end_time = pd.Timestamp(day)\n",
        "\n",
        "        serie_tgrt_filt = serie_tgrt.slice(min_time, start_real)\n",
        "        serie_tgrt_test_filt = serie_tgrt_test.slice(start_time, end_time)\n",
        "        current_series = serie_tgrt_filt\n",
        "        predictions_05, predictions_median, predictions_95 = [], [], []\n",
        "\n",
        "        if model.uses_past_covariates or model.uses_future_covariates:\n",
        "\n",
        "            # Predicts\n",
        "            pred = model.predict(\n",
        "                series=current_series,\n",
        "                past_covariates=series_cov,\n",
        "                future_covariates=series_cov[future_variables],\n",
        "                n=1,\n",
        "                predict_likelihood_parameters=True)\n",
        "            predictions_05.append(pred[f'{target_col}_q0.05'].values().item())\n",
        "            predictions_median.append(pred[f'{target_col}_q0.50'].values().item())\n",
        "            predictions_95.append(pred[f'{target_col}_q0.95'].values().item())\n",
        "            current_series = current_series.append(pred[f'{target_col}_q0.50'])\n",
        "        else:\n",
        "            pred = model.predict(series=serie_tgrt_filt, n=len(serie_tgrt_test_filt))\n",
        "            predictions_median = pred.values().flatten()\n",
        "\n",
        "        pred_05_series = TimeSeries.from_times_and_values(serie_tgrt_test_filt.time_index, np.array(predictions_05))\n",
        "        pred_series = TimeSeries.from_times_and_values(serie_tgrt_test_filt.time_index, np.array(predictions_median))\n",
        "        pred_95_series = TimeSeries.from_times_and_values(serie_tgrt_test_filt.time_index, np.array(predictions_95))\n",
        "\n",
        "        if scale_trgt:\n",
        "            pred_05_series = transformer_trgt.inverse_transform(pred_05_series)\n",
        "            pred_series = transformer_trgt.inverse_transform(pred_series)\n",
        "            pred_95_series = transformer_trgt.inverse_transform(pred_95_series)\n",
        "            serie_tgrt_test_filt = transformer_trgt.inverse_transform(serie_tgrt_test_filt)\n",
        "\n",
        "        pred_05_series = pred_05_series.map(replace_negatives_with_zero)\n",
        "        pred_series = pred_series.map(replace_negatives_with_zero)\n",
        "        pred_95_series = pred_95_series.map(replace_negatives_with_zero)\n",
        "\n",
        "        if start_time.weekday() < 5:\n",
        "            df_trgt = pred_series.pd_dataframe()\n",
        "            if df_trgt.index.weekday.unique() < 5:\n",
        "                mape = metrics.mape(serie_tgrt_test_filt + 1, pred_series + 1)\n",
        "                smape = metrics.smape(serie_tgrt_test_filt + 1, pred_series + 1)\n",
        "                ope = metrics.ope(serie_tgrt_test_filt + 1, pred_series + 1)\n",
        "                r2 = metrics.r2_score(serie_tgrt_test_filt + 1, pred_series + 1)\n",
        "                rmse = metrics.rmse(serie_tgrt_test_filt + 1, pred_series + 1)\n",
        "                wape = calculate_wape(serie_tgrt_test_filt + 1, pred_series + 1)\n",
        "\n",
        "                metrics_lists['mape'].append(mape)\n",
        "                metrics_lists['smape'].append(smape)\n",
        "                metrics_lists['ope'].append(ope)\n",
        "                metrics_lists['r2'].append(r2)\n",
        "                metrics_lists['rmse'].append(rmse)\n",
        "                metrics_lists['wape'].append(wape.values().squeeze().item())\n",
        "\n",
        "        df_preds = pd.concat([df_preds, \n",
        "                              pred_05_series.stack(pred_series).stack(pred_95_series).pd_dataframe().rename(\n",
        "                                  columns={'0': 'NET_VALUE_predicted_05', '0_1': 'NET_VALUE_predicted', '0_1_1': 'NET_VALUE_predicted_95'})])\n",
        "\n",
        "    return df_preds, metrics_lists"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n",
        "## 2. Data Import and Preparation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Sales Series\n",
        "str_select_sales = '''\n",
        "                        SELECT *\n",
        "                        FROM analytics.refined_sales_orders_agg\n",
        "                        '''\n",
        "\n",
        "df_series_sales = spark.sql(str_select_sales).toPandas()\n",
        "\n",
        "df_series_sales.info()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Setting SYSTEM_TIMESTAMP column as DataFrame index\n",
        "df_series_sales.set_index('SYSTEM_TIMESTAMP', inplace=True)\n",
        "df_series_sales.sort_index(inplace=True)\n",
        "\n",
        "# Converts granularity to Daily\n",
        "df_series_sales_resampled = df_series_sales.resample('D').sum()\n",
        "\n",
        "\n",
        "\n",
        "########## Outlier Treatment   ##########\n",
        "# Decomposing the series for residual outlier verification\n",
        "result = seasonal_decompose(df_series_sales_resampled['NET_VALUE'], model='additive', period=8)\n",
        "trend = result.trend\n",
        "seasonal = result.seasonal\n",
        "residual = result.resid\n",
        "\n",
        "# Calculates residual z-scores\n",
        "z_scores = stats.zscore(residual.dropna())\n",
        "\n",
        "# Gets outlier indices\n",
        "zscore_threshold = 3 # Since the series has seasonality and high amplitude, a lower threshold would cut natural peaks\n",
        "outliers = np.abs(z_scores) > zscore_threshold\n",
        "outliers_index = residual.dropna().index[outliers]\n",
        "\n",
        "# Residual interpolation at outlier points\n",
        "residual_adjusted = residual.copy()\n",
        "residual_adjusted[outliers_index] = np.nan\n",
        "residual_adjusted = residual_adjusted.interpolate()\n",
        "residual_adjusted = residual_adjusted.fillna(method='bfill').fillna(method='ffill')\n",
        "\n",
        "# Reconstruction of adjusted time series\n",
        "adjusted_time_series = trend + seasonal + residual_adjusted\n",
        "df_series_sales_resampled['NET_VALUE_clean'] = adjusted_time_series\n",
        "df_series_sales_resampled['NET_VALUE_clean'] = df_series_sales_resampled['NET_VALUE_clean'].fillna(df_series_sales_resampled['NET_VALUE'])\n",
        "\n",
        "\n",
        "\n",
        "# Creating calendar variables\n",
        "df_series_sales_resampled['year'] = df_series_sales_resampled.index.year\n",
        "df_series_sales_resampled['month'] = df_series_sales_resampled.index.month\n",
        "df_series_sales_resampled['week_of_year'] = df_series_sales_resampled.index.isocalendar().week\n",
        "df_series_sales_resampled['day_of_week'] = df_series_sales_resampled.index.day_of_week #  Monday=0 and Sunday=6\n",
        "df_series_sales_resampled['day_of_month'] = df_series_sales_resampled.index.day\n",
        "\n",
        "\n",
        "# Inspecting\n",
        "df_series_sales_resampled.info()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "display(df_series_sales_resampled.reset_index())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# The operation below is not necessary, as model optimization showed such variables do not help with current parameterization.\n",
        "# The code will be kept here in case it is needed at some future time.\n",
        "'''\n",
        "# Including holiday\n",
        "brazil_holidays = holidays.Brazil()\n",
        "df_series_sales_resampled['event'] = df_series_sales_resampled.index.map(lambda x: brazil_holidays.get(x, None))\n",
        "\n",
        "# Adding custom holidays\n",
        "df_series_sales_resampled['event'] = df_series_sales_resampled.apply(include_events, axis=1)\n",
        "\n",
        "# Creating holiday flag\n",
        "df_series_sales_resampled['event_flag'] = df_series_sales_resampled['event'].apply(lambda x: 1 if x is not None else 0)\n",
        "\n",
        "df_series_sales_resampled['event'].fillna('Normal Day', inplace=True)\n",
        "'''"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Holidays to consider if dummies are created\n",
        "events_to_consider = None\n",
        "\n",
        "# Numerical variables\n",
        "numeric_features = ['day_of_week', 'day_of_month', 'month']\n",
        "future_variables = ['day_of_week', 'day_of_month', 'month']\n",
        "\n",
        "# Generate day of week dummy\n",
        "dummy_day_of_week = False\n",
        "\n",
        "# Use event binary flag or not\n",
        "use_event_flag = False\n",
        "\n",
        "# Scale target series or not\n",
        "scale_trgt = True\n",
        "\n",
        "# Scale covariate series or not\n",
        "scale_cov = False\n",
        "\n",
        "# Scaling method (minmax or power)\n",
        "scale_method = 'minmax'\n",
        "\n",
        "# Target series column to predict\n",
        "target_col = 'NET_VALUE'\n",
        "\n",
        "# Generating series\n",
        "return_list = prepare_input(df=df_series_sales_resampled, \n",
        "                            target_col=target_col,\n",
        "                            numeric_features=numeric_features, \n",
        "                            use_event_flag=use_event_flag, \n",
        "                            events_to_consider=events_to_consider,\n",
        "                            scale_trgt=scale_trgt,\n",
        "                            scale_cov=scale_cov,\n",
        "                            scale_method=scale_method,\n",
        "                            dummy_day_of_week=dummy_day_of_week)\n",
        "\n",
        "# Extracting objects from return list\n",
        "serie_tgrt = return_list[0]\n",
        "if use_event_flag or events_to_consider or numeric_features:\n",
        "    series_cov = return_list[1]\n",
        "if scale_trgt:\n",
        "    transformer_trgt = return_list[2]\n",
        "\n",
        "# Saving scaler to disk\n",
        "artifacts_root = '/Workspace/Repos/DataScience/FORECAST_PROJECT/src/artifacts/'\n",
        "with open(f'{artifacts_root}transformer_trgt.pkl', 'wb') as f:\n",
        "    pickle.dump(transformer_trgt, f)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n",
        "## 4. Training, Evaluation and Registration"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Setting experiment\n",
        "name = 'forecast_sales_daily'\n",
        "model_name = 'LightGBM'\n",
        "registered_model_name = 'LightGBM_forecast_sales_daily'\n",
        "experiment_name = f\"/Users/data.scientist@company.com/{name}_training\"\n",
        "mlflow.set_experiment(experiment_name)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Separating train and test\n",
        "test_pct = 0.2 # Percentage of test sample\n",
        "n_test = int(len(serie_tgrt) * test_pct)\n",
        "dt_test_start = (serie_tgrt.time_index.max() - timedelta(days=n_test)).replace(hour=0, minute=0, second=0)\n",
        "serie_tgrt_train, serie_tgrt_test = serie_tgrt.split_before(dt_test_start)\n",
        "if use_event_flag or events_to_consider or numeric_features:\n",
        "    serie_cov_train, serie_cov_test = series_cov.split_before(dt_test_start)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Running the model\n",
        "with mlflow.start_run() as mlflow_run:\n",
        "    # Model parameters - already adjusted in optimization during model development\n",
        "    lags = [-175, -168, -161, -154, -147, -140, -133, -126, -119, -112, -105, -98, -92, -91, -84, -77, -70, -63, -62, -56, -49, -42, -41, -37, -35, -34, -29, -28, -27, -26, -23, -22, -21, -20, -19, -16, -15, -14, -13, -12, -9, -8, -7, -6, -5, -2, -1]\n",
        "    lags_past_covariates = lags\n",
        "    lags_future_covariates = [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15]\n",
        "    output_chunk_length = 1\n",
        "    random_seed = 42\n",
        "    n_estimators = 5000\n",
        "    multi_models = False # If true, creates separate models to predict each future step. These models are encapsulated in the single final model.\n",
        "\n",
        "    use_covariates = use_event_flag or events_to_consider or numeric_features\n",
        "   \n",
        "    # Model Creation and Training\n",
        "    if use_covariates:\n",
        "        quantiles = [0.05, 0.5, 0.95] # For a 90% confidence interval\n",
        "        model = LightGBMModel(\n",
        "                                lags=lags,\n",
        "                                output_chunk_length=output_chunk_length,\n",
        "                                lags_past_covariates=lags_past_covariates,\n",
        "                                lags_future_covariates=lags_future_covariates,\n",
        "                                random_state=random_seed,\n",
        "                                n_estimators=n_estimators,\n",
        "                                multi_models=multi_models,\n",
        "                                force_row_wise=False,\n",
        "                                force_col_wise=False,\n",
        "                                #boosting_type='dart',\n",
        "                                #num_leaves=40,\n",
        "                                #max_depth=0,\n",
        "                                learning_rate=None,\n",
        "                                likelihood='quantile', \n",
        "                                quantiles=quantiles)\n",
        "        model.fit(series=serie_tgrt_train, past_covariates=series_cov, future_covariates=series_cov[future_variables])\n",
        "    else:\n",
        "        model = create_model(use_covariates, lags, output_chunk_length, random_seed, n_estimators, multi_models)\n",
        "        model.fit(series=serie_tgrt)\n",
        "        \n",
        "    # If using positive lags for future covariates\n",
        "    if len(lags_future_covariates) > 1:\n",
        "        serie_tgrt_test_cut = serie_tgrt_test[:-max(lags_future_covariates)]\n",
        "\n",
        "    # Prediction and Validation\n",
        "    if scale_trgt:\n",
        "        df_preds, metrics_lists = predict_and_evaluate(model, serie_tgrt, serie_tgrt_test_cut, series_cov, scale_trgt, transformer_trgt, target_col)\n",
        "    else:\n",
        "        df_preds, mape_list, smape_list, ope_list, r2_list, rmse_list, wape_list, mape_list_acc, smape_list_acc,\\\n",
        "        ope_list_acc, r2_list_acc, rmse_list_acc, wape_list_acc = predict_and_evaluate(model, serie_tgrt, serie_tgrt_test_cut, series_cov, scale_trgt, None, target_col)\n",
        "\n",
        "    # Adding actual values to dataframe with predictions\n",
        "    if scale_trgt:\n",
        "        serie_tgrt_test = transformer_trgt.inverse_transform(serie_tgrt_test)\n",
        "    df_preds['NET_VALUE_real'] = serie_tgrt_test.pd_dataframe()[[target_col]]\n",
        "    df_preds = df_preds.reset_index(drop=False).rename(columns={'index': 'SYSTEM_TIMESTAMP'})\n",
        "\n",
        "    # Creating metrics dataframe per predicted day, disregarding weekends\n",
        "    df_metrics = pd.DataFrame({'date': df_preds[df_preds.SYSTEM_TIMESTAMP.dt.weekday < 5].SYSTEM_TIMESTAMP,\n",
        "                                'mape': metrics_lists['mape'],\n",
        "                                'smape': metrics_lists['smape'],\n",
        "                                'ope': metrics_lists['ope'],\n",
        "                                'r2': metrics_lists['r2'],\n",
        "                                'rmse': metrics_lists['rmse'],\n",
        "                                'wape': metrics_lists['wape']})\n",
        "    df_metrics['date'] = pd.to_datetime(df_metrics['date'])\n",
        "\n",
        "    # Logging model parameters in MLflow\n",
        "    mlflow.log_params({\n",
        "        \"n_test\": n_test,\n",
        "        \"lags\": lags,\n",
        "        \"output_chunk_length\": output_chunk_length,\n",
        "        \"random_seed\": random_seed,\n",
        "        \"n_estimators\": n_estimators,\n",
        "        \"multi_models\": multi_models,\n",
        "        \"dt_test_start\": serie_tgrt_test[0].time_index.min(),\n",
        "        \"dt_test_end\": serie_tgrt_test[0].time_index.max(),\n",
        "        \"scale_trgt\": scale_trgt,\n",
        "        \"scale_method\": scale_method,\n",
        "        \"dummy_day_of_week\": dummy_day_of_week,\n",
        "        \"model_name\": model_name\n",
        "    })\n",
        "    if use_covariates:\n",
        "        mlflow.log_params({\n",
        "            \"lags_past_covariates\": lags_past_covariates,\n",
        "            \"lags_future_covariates\": lags_future_covariates,\n",
        "            \"scale_cov\": scale_cov,\n",
        "            \"use_event_flag\": use_event_flag,\n",
        "            \"numeric_features\": numeric_features,\n",
        "            \"events_to_consider\": events_to_consider\n",
        "        })\n",
        "\n",
        "    # Logging metrics mean and median\n",
        "    mlflow.log_metrics({\n",
        "        \"mape_median\": np.median(metrics_lists['mape']),\n",
        "        \"smape_median\": np.median(metrics_lists['smape']),\n",
        "        \"ope_median\": np.median(metrics_lists['ope']),\n",
        "        \"r2_median\": np.median(metrics_lists['r2']),\n",
        "        \"rmse_median\": np.median(metrics_lists['rmse']),\n",
        "        \"wape_median\": np.median(metrics_lists['wape']),\n",
        "        \"mape_avg\": np.average(metrics_lists['mape']),\n",
        "        \"smape_avg\": np.average(metrics_lists['smape']),\n",
        "        \"ope_avg\": np.average(metrics_lists['ope']),\n",
        "        \"r2_avg\": np.average(metrics_lists['r2']),\n",
        "        \"rmse_avg\": np.average(metrics_lists['rmse']),\n",
        "        \"wape_avg\": np.average(metrics_lists['wape'])\n",
        "    })\n",
        "\n",
        "    # Logging prediction and metrics results\n",
        "    df_preds.to_csv(artifacts_root + f'predictions_{name}.csv', index=False)\n",
        "    mlflow.log_artifact(artifacts_root + f\"predictions_{name}.csv\")\n",
        "    df_metrics.to_csv(artifacts_root + f'metrics_{name}.csv', index=False)\n",
        "    mlflow.log_artifact(artifacts_root + f\"metrics_{name}.csv\")\n",
        "    \n",
        "    # Plot - Metrics Time Series for MAPE\n",
        "    fig, ax = plt.subplots(figsize=(25, 10))\n",
        "    df_metrics[['date', 'wape']].set_index('date').plot(ax=ax)\n",
        "    plt.title('WAPE Time Series', fontsize=20, loc='center', weight='bold')\n",
        "    ax.xaxis.set_major_locator(plt.MaxNLocator(15))\n",
        "    ax.tick_params(axis='x', rotation=45)\n",
        "    plt.savefig(artifacts_root + f'ts_metrics_{name}.png', bbox_inches='tight')\n",
        "    mlflow.log_artifact(artifacts_root + f\"ts_metrics_{name}.png\")\n",
        "\n",
        "    # Creating WAPE chart\n",
        "    plt.figure(figsize=(15, 10))\n",
        "    plt.title(f'WAPE Histogram - {name} - median:{round(np.median(metrics_lists[\"wape\"]), 2)}')\n",
        "    sns.histplot(remove_outliers_zscore(metrics_lists['wape']), kde = False)\n",
        "    plt.axvline(x = np.median(remove_outliers_zscore(metrics_lists['wape'])), linestyle = '--')\n",
        "    plt.xlabel(\"WAPE\")\n",
        "    plt.savefig(artifacts_root + f\"hist_wape_{name}.png\")\n",
        "    mlflow.log_artifact(artifacts_root + f\"hist_wape_{name}.png\") \n",
        "\n",
        "    # Creating OPE chart\n",
        "    plt.figure(figsize=(15, 10))\n",
        "    plt.title(f'OPE Histogram - {name} - median:{round(np.median(metrics_lists[\"ope\"]), 2)}')\n",
        "    sns.histplot(remove_outliers_zscore(metrics_lists['ope']), kde = False)\n",
        "    plt.axvline(x = np.median(remove_outliers_zscore(metrics_lists['ope'])), linestyle = '--')\n",
        "    plt.xlabel(\"OPE\")\n",
        "    plt.savefig(artifacts_root + f\"hist_ope_{name}.png\")\n",
        "    mlflow.log_artifact(artifacts_root + f\"hist_ope_{name}.png\") \n",
        "\n",
        "    # Calculating and logging prediction and actual decomposition\n",
        "    decomposition_real = seasonal_decompose(df_preds[['NET_VALUE_real']], \n",
        "                                        model = 'additive', \n",
        "                                        period = 30,\n",
        "                                        two_sided = False)\n",
        "\n",
        "    decomposition_predicted = seasonal_decompose(df_preds[['NET_VALUE_predicted']], \n",
        "                                                model = 'additive', \n",
        "                                                period = 30,\n",
        "                                                two_sided = False)\n",
        "\n",
        "    df_components = pd.DataFrame(decomposition_real.trend)\n",
        "    df_components.columns = ['trend_hist']\n",
        "    df_components['seasonal_hist'] = decomposition_real.seasonal\n",
        "    df_components['observed_hist'] = decomposition_real.observed\n",
        "    df_components['resid_hist'] = decomposition_real.resid\n",
        "    df_components['trend_pred'] = decomposition_predicted.trend\n",
        "    df_components['seasonal_pred'] = decomposition_predicted.seasonal\n",
        "    df_components['observed_pred'] = decomposition_predicted.observed\n",
        "    df_components['resid_pred'] = decomposition_predicted.resid\n",
        "    df_components.index = df_preds['SYSTEM_TIMESTAMP']\n",
        "\n",
        "    fig, axes = plt.subplots(nrows=4, ncols=1, figsize=(20, 12), sharex = True)\n",
        "    fig.suptitle(f'Sellout and predicted components - {name}')\n",
        "    df_components = df_components[-8*7:]\n",
        "    df_components[['observed_hist', 'observed_pred']].plot(ax=axes[0], title = 'Observed')\n",
        "    df_components[['trend_hist', 'trend_pred']].plot(ax=axes[1], title = 'Trend')\n",
        "    df_components[['seasonal_hist', 'seasonal_pred']].plot(ax=axes[2], title = 'Seasonal')\n",
        "    df_components[['resid_hist', 'resid_pred']].plot(ax=axes[3], title = 'Residual')\n",
        "    fig.savefig(artifacts_root + f\"components_real_predict_{name}.png\")\n",
        "    mlflow.log_artifact(artifacts_root + f\"components_real_predict_{name}.png\") \n",
        "\n",
        "    # Calculating and logging residuals\n",
        "    df_preds['residuals'] = df_preds.NET_VALUE_predicted - df_preds.NET_VALUE_real\n",
        "    plt.figure(figsize=(15, 10))\n",
        "    plt.title(f'Residuals Histogram - {name} - median:{round(df_preds.residuals.median(), 2)}')\n",
        "    sns.histplot(df_preds, x = 'residuals', kde = False)\n",
        "    plt.axvline(x = df_preds.residuals.mean(), linestyle = '--')\n",
        "    plt.xlabel(\"Residuals\")\n",
        "    plt.savefig(artifacts_root + f\"hist_residuals_{name}.png\")\n",
        "    mlflow.log_artifact(artifacts_root + f\"hist_residuals_{name}.png\") \n",
        "\n",
        "    # Creating chart with residual autocorrelation\n",
        "    plt.rc(\"figure\",figsize=(20,7))\n",
        "    acf = plot_acf(df_preds.residuals, title=f'Residuals Autocorrelation - {name}');\n",
        "    acf.savefig(artifacts_root + f\"ACF_residuals_agg_{name}.png\")\n",
        "    mlflow.log_artifact(artifacts_root + f\"ACF_residuals_agg_{name}.png\")\n",
        "\n",
        "    # Verifying and logging residual autocorrelation\n",
        "    # If pvalue is greater than 0.05, H0 is not rejected. H0: the model does not exhibit lack of fit\n",
        "    # H0 suggests the model captured the autocorrelation structure present in data\n",
        "    # Including pvalue_ljungbox_agg in metrics and logging\n",
        "    resids = df_preds.residuals.values\n",
        "    pvalue_ljungbox = round(acorr_ljungbox(x = resids).lb_pvalue.max(), 4) \n",
        "    mlflow.log_metric(\"pvalue_ljungbox\", pvalue_ljungbox)\n",
        "\n",
        "\n",
        "    # Performing training with all available data\n",
        "    if use_covariates:\n",
        "        quantiles = [0.05, 0.5, 0.95] # For a 90% confidence interval\n",
        "        model = LightGBMModel(\n",
        "                                lags=lags,\n",
        "                                output_chunk_length=output_chunk_length,\n",
        "                                lags_past_covariates=lags_past_covariates,\n",
        "                                lags_future_covariates=lags_future_covariates,\n",
        "                                random_state=random_seed,\n",
        "                                n_estimators=n_estimators,\n",
        "                                multi_models=multi_models,\n",
        "                                force_row_wise=False,\n",
        "                                force_col_wise=False,\n",
        "                                #boosting_type='dart',\n",
        "                                #num_leaves=40,\n",
        "                                #max_depth=0,\n",
        "                                learning_rate=None,\n",
        "                                likelihood='quantile', \n",
        "                                quantiles=quantiles)\n",
        "        model.fit(series=serie_tgrt, past_covariates=series_cov, future_covariates=series_cov[future_variables])\n",
        "    else:\n",
        "        model = create_model(use_covariates, lags, output_chunk_length, random_seed, n_estimators, multi_models)\n",
        "        model.fit(series=serie_tgrt)\n",
        "\n",
        "    # Logging final model\n",
        "    mlflow.sklearn.log_model(model, model_name+'_'+name)\n",
        "    \n",
        "    # Registering model in Model Registry\n",
        "    run_id = mlflow_run.info.run_id\n",
        "    model_uri = f\"runs:/{run_id}/{registered_model_name}\"\n",
        "    registered_model = mlflow.register_model(model_uri, registered_model_name)\n",
        "\n",
        "    mlflow.end_run()"
      ],
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 4
}