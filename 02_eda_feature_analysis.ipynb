{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Databricks notebook source\n",
        "\n",
        "## 1. Libraries"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Analysis\n",
        "from statsmodels.tsa.seasonal import seasonal_decompose\n",
        "from scipy.stats import variation\n",
        "from statsmodels.stats.diagnostic import acorr_ljungbox\n",
        "from statsmodels.graphics.tsaplots import plot_acf, plot_pacf\n",
        "from statsmodels.tsa.stattools import acf, pacf\n",
        "from scipy import stats\n",
        "from scipy.stats import f_oneway\n",
        "from scipy.stats import shapiro\n",
        "from statsmodels.tsa.stattools import adfuller\n",
        "from scipy.stats import pearsonr, spearmanr\n",
        "from sklearn.preprocessing import MinMaxScaler, PowerTransformer\n",
        "\n",
        "# Utilities\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import datetime\n",
        "from dateutil.relativedelta import relativedelta\n",
        "from datetime import datetime, timedelta\n",
        "import holidays\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "import pywt\n",
        "from scipy.fftpack import fft\n",
        "\n",
        "# Plotting\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import plotly.graph_objects as go\n",
        "import plotly.express as ply\n",
        "\n",
        "# Notebook configuration\n",
        "%matplotlib inline\n",
        "%config InlineBackend.figure_format = 'retina'\n",
        "sns.set_theme(style=\"whitegrid\", palette=\"pastel\")\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n",
        "## 2. Data Import"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Monthly Budget\n",
        "df_monthly_budget = pd.read_excel('/Workspace/Repos/DataScience/FORECAST_PROJECT/src/data/2024_Budget_Revenue_Adjusted.xlsx',\n",
        "                   sheet_name='Target_Adjustment', skiprows=2, header=None, usecols=\"AR:BE\", nrows=6)\n",
        "df_monthly_budget.columns = df_monthly_budget.iloc[0]\n",
        "df_monthly_budget = df_monthly_budget[1:]\n",
        "df_monthly_budget = df_monthly_budget[df_monthly_budget['BU'] == 'Total'].transpose().reset_index()[1:-1]\n",
        "df_monthly_budget.columns = ['month', 'total']\n",
        "\n",
        "df_monthly_budget['total'] = df_monthly_budget['total'].astype(float)\n",
        "\n",
        "df_monthly_budget.info()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Sales Series\n",
        "str_select_sales = '''\n",
        "                        SELECT *\n",
        "                        FROM analytics.refined_sales_orders_agg\n",
        "                        '''\n",
        "\n",
        "df_series_sales = spark.sql(str_select_sales).toPandas()\n",
        "\n",
        "df_series_sales.info()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "df_series_sales.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n",
        "## 3. Feature Analysis"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "'''\n",
        "General\n",
        "  Distribution\n",
        "  Outliers\n",
        "  Components\n",
        "  ACF/PACF\n",
        "  Day of month\n",
        "  Day of week\n",
        "  Holidays\n",
        "  Monthly budget\n",
        "\n",
        "Hourly cumulative processed series\n",
        "  Distribution  \n",
        "  Outliers\n",
        "  Components\n",
        "  ACF/PACF\n",
        "  Day of month\n",
        "  Day of week\n",
        "  Holidays\n",
        "  Monthly budget\n",
        "\n",
        "Project cumulatives or 30-minute granularity series????\n",
        "Project components?\n",
        "Project stationary series?\n",
        "\n",
        "'''"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n",
        "### 3.1 30-min Granularity Series"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n",
        "#### 3.1.1 General Parameters"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Setting SYSTEM_TIMESTAMP column as DataFrame index\n",
        "df_series_sales.set_index('SYSTEM_TIMESTAMP', inplace=True)\n",
        "df_series_sales.sort_index(inplace=True)\n",
        "\n",
        "# Converting granularity to 30 minutes\n",
        "df_series_sales_resampled = df_series_sales.resample('30T').sum()\n",
        "\n",
        "# Creating a date range with 30-minute granularity between start and end date\n",
        "date_range = pd.date_range(start=df_series_sales.index.min().strftime('%Y-%m-%d %H:00:00'), end=df_series_sales.index.max().strftime('%Y-%m-%d %H:30:00'), freq='30T')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "df_series_sales_resampled.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Checking if original data has all time points\n",
        "date_range.difference(df_series_sales_resampled.index).shape[0]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "df_series_sales_resampled.describe()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "display(df_series_sales_resampled.reset_index()[-50:])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "df_series_sales_resampled.plot.line(figsize=(20, 7))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "plt.figure(figsize=(10, 7))\n",
        "sns.histplot(df_series_sales_resampled)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "plt.figure(figsize=(10, 7))\n",
        "sns.boxplot(df_series_sales_resampled, showfliers=False)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Calculating zscore to check for outliers\n",
        "df_series_sales_resampled['zscore'] = stats.zscore(df_series_sales_resampled[['NET_VALUE']])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "df_series_sales_resampled.describe()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Series without outliers\n",
        "display(df_series_sales_resampled[df_series_sales_resampled.zscore.abs() < 3].reset_index())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Distribution without outliers\n",
        "plt.figure(figsize=(10, 7))\n",
        "sns.histplot(df_series_sales_resampled[df_series_sales_resampled.zscore.abs() < 3][['NET_VALUE']])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# How many points are zero\n",
        "f'{round(len(df_series_sales_resampled[df_series_sales_resampled.NET_VALUE == 0]) / len(df_series_sales_resampled), 2) * 100}%'"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Shapiro-Wilk normality test\n",
        "statistic, p_value = shapiro(df_series_sales_resampled['NET_VALUE'].values)\n",
        "\n",
        "if p_value < 0.05:\n",
        "    print('The series does not follow a normal curve.')    \n",
        "else:\n",
        "    print('The series follows a normal curve.\\n')    \n",
        "print('Test Statistic:', statistic)\n",
        "print('P-value:', p_value)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Creating box-cox scaled values\n",
        "boxcox_transformer = PowerTransformer()\n",
        "df_series_sales_resampled['NET_VALUE_boxcox'] = boxcox_transformer.fit_transform(df_series_sales_resampled[['NET_VALUE']])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Checking box-cox distribution\n",
        "plt.figure(figsize=(10, 7))\n",
        "sns.histplot(df_series_sales_resampled['NET_VALUE_boxcox'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Shapiro-Wilk normality test for box-cox\n",
        "statistic, p_value = shapiro(df_series_sales_resampled['NET_VALUE_boxcox'])\n",
        "\n",
        "if p_value < 0.05:\n",
        "    print('The series does not follow a normal curve.')    \n",
        "else:\n",
        "    print('The series follows a normal curve.\\n')    \n",
        "print('Test Statistic:', statistic)\n",
        "print('P-value:', p_value)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Checking distribution for differenced values\n",
        "plt.figure(figsize=(10, 7))\n",
        "sns.histplot(df_series_sales_resampled['NET_VALUE'].diff())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Shapiro-Wilk normality test for differenced series\n",
        "statistic, p_value = shapiro(df_series_sales_resampled['NET_VALUE'].diff())\n",
        "\n",
        "if p_value < 0.05:\n",
        "    print('The series does not follow a normal curve.')    \n",
        "else:\n",
        "    print('The series follows a normal curve.\\n')    \n",
        "print('Test Statistic:', statistic)\n",
        "print('P-value:', p_value)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Stationarity Test\n",
        "# Stationarity Test - Required for Granger Causality Test\n",
        "stationarity_test_result = adfuller(df_series_sales_resampled[['NET_VALUE']])\n",
        "stationarity_test_result\n",
        "\n",
        "'''\n",
        "return:\n",
        "\n",
        "(test statistic,\n",
        "pvalue,\n",
        "number of lags used,\n",
        "number of observations used for regression and calculation of critical values),\n",
        "{critical values for different significance levels},\n",
        "maximized information criterion\n",
        "'''"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Stationarity Test\n",
        "if stationarity_test_result[1] <= 0.05:\n",
        "    print(f'The series is stationary')\n",
        "    print(f'p-value: {round(stationarity_test_result[1], 5)}')\n",
        "else:\n",
        "    print(f'The series is NOT stationary')\n",
        "    print(f'p-value: {round(stationarity_test_result[1], 5)}')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Plotting time series decomposition\n",
        "decomposition = seasonal_decompose(df_series_sales_resampled[['NET_VALUE']], \n",
        "                                  model = 'additive', \n",
        "                                  period = 7,\n",
        "                                  two_sided = True)\n",
        "\n",
        "fig_observed = go.Figure()\n",
        "fig_trend = go.Figure()\n",
        "fig_seasonal = go.Figure()\n",
        "fig_residual = go.Figure()\n",
        "\n",
        "fig_observed.add_trace(go.Scatter(x=decomposition.observed.index, y=decomposition.observed.values, name='Observed'))\n",
        "fig_trend.add_trace(go.Scatter(x=decomposition.trend.index, y=decomposition.trend.values, name='Trend'))\n",
        "fig_seasonal.add_trace(go.Scatter(x=decomposition.seasonal.index, y=decomposition.seasonal.values, name='Seasonality'))\n",
        "fig_residual.add_trace(go.Scatter(x=decomposition.resid.index, y=decomposition.resid.values, name='Residual'))\n",
        "\n",
        "fig_observed.update_layout(title='Observed')\n",
        "fig_trend.update_layout(title='Trend')\n",
        "fig_seasonal.update_layout(title='Seasonality')\n",
        "fig_residual.update_layout(title='Residual')\n",
        "\n",
        "fig_observed.show()\n",
        "fig_trend.show()\n",
        "fig_seasonal.show()\n",
        "fig_residual.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "sns.histplot(decomposition.resid.values)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Shapiro-Wilk normality test for residuals\n",
        "statistic, p_value = shapiro(decomposition.resid.dropna().values)\n",
        "\n",
        "if p_value < 0.05:\n",
        "    print('The series does not follow a normal curve.')    \n",
        "else:\n",
        "    print('The series follows a normal curve.\\n')    \n",
        "print('Test Statistic:', statistic)\n",
        "print('P-value:', p_value)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Residual stationarity test\n",
        "stationarity_test_result = adfuller(decomposition.resid.dropna().values)\n",
        "stationarity_test_result\n",
        "\n",
        "if stationarity_test_result[1] <= 0.05:\n",
        "    print(f'The series is stationary')\n",
        "    print(f'p-value: {round(stationarity_test_result[1], 5)}')\n",
        "else:\n",
        "    print(f'The series is NOT stationary')\n",
        "    print(f'p-value: {round(stationarity_test_result[1], 5)}')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n",
        "#### 3.1.2 Seasonalities"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Analyzing frequencies present in the series\n",
        "\n",
        "# Scaling data\n",
        "scale = MinMaxScaler()\n",
        "df_series_sales_resampled['net_value_scld'] = scale.fit_transform(df_series_sales_resampled[['NET_VALUE']]).squeeze()\n",
        "\n",
        "\n",
        "# Functions for signal analysis\n",
        "def get_ave_values(xvalues, yvalues, n = 5):\n",
        "    signal_length = len(xvalues)\n",
        "    if signal_length % n == 0:\n",
        "        padding_length = 0\n",
        "    else:\n",
        "        padding_length = n - signal_length//n % n\n",
        "    xarr = np.array(xvalues)\n",
        "    yarr = np.array(yvalues)\n",
        "    xarr.resize(signal_length//n, n)\n",
        "    yarr.resize(signal_length//n, n)\n",
        "    xarr_reshaped = xarr.reshape((-1,n))\n",
        "    yarr_reshaped = yarr.reshape((-1,n))\n",
        "    x_ave = xarr_reshaped[:,0]\n",
        "    y_ave = np.nanmean(yarr_reshaped, axis=1)\n",
        "    return x_ave, y_ave\n",
        "\n",
        "def plot_signal_plus_average(time, signal, average_over = 5):\n",
        "    fig, ax = plt.subplots(figsize=(20, 5))\n",
        "    time_ave, signal_ave = get_ave_values(time, signal, average_over)\n",
        "    ax.plot(time, signal, label='signal')\n",
        "    ax.plot(time_ave, signal_ave, label = 'time average (n={})'.format(5))\n",
        "    ax.set_xlim([time[0], time[-1]])\n",
        "    ax.set_ylabel('Signal Amplitude', fontsize=18)\n",
        "    ax.set_title('Signal + Time Average', fontsize=18)\n",
        "    ax.set_xlabel('Time', fontsize=18)\n",
        "    ax.legend()\n",
        "    plt.show()\n",
        "\n",
        "def get_fft_values(y_values, T, N, f_s):\n",
        "    f_values = np.linspace(0.0, 1.0/(2.0*T), N//2)\n",
        "    fft_values_ = fft(y_values)\n",
        "    fft_values = 2.0/N * np.abs(fft_values_[0:N//2])\n",
        "    return f_values, fft_values\n",
        "\n",
        "def plot_fft_plus_power(time, signal, n=None):\n",
        "    dt = time[1] - time[0]\n",
        "    N = len(signal)\n",
        "    fs = 1/dt\n",
        "\n",
        "    fig, ax = plt.subplots(figsize=(20, 5))\n",
        "    variance = np.std(signal)**2\n",
        "    f_values, fft_values = get_fft_values(signal, dt, N, fs)\n",
        "    fft_power = variance * abs(fft_values) ** 2     # FFT power spectrum\n",
        "    if n:\n",
        "        ax.plot(f_values[:n], fft_values[:n], 'r-', label='Fourier Transform')\n",
        "\n",
        "        # X-axis ticks adjustment\n",
        "        num_ticks = 20  \n",
        "        ax.set_xticks(np.linspace(f_values[:n][0], f_values[:n][-1], num_ticks))\n",
        "    else:\n",
        "        ax.plot(f_values, fft_values, 'r-', label='Fourier Transform')\n",
        "        # X-axis ticks adjustment\n",
        "        num_ticks = 20\n",
        "        ax.set_xticks(np.linspace(f_values[0], f_values[-1], num_ticks))\n",
        "\n",
        "\n",
        "    #ax.plot(f_values, fft_power, 'k--', linewidth=1, label='FFT Power Spectrum')\n",
        "    ax.set_xlabel('Frequency [Hz / hour]', fontsize=18)\n",
        "    ax.set_ylabel('Amplitude', fontsize=18)\n",
        "    ax.legend()\n",
        "    plt.show()\n",
        "\n",
        "\n",
        "def plot_wavelet_serie(time, signal, scales,\n",
        "                        waveletname='cmor',\n",
        "                        #cmap=plt.cm.seismic,\n",
        "                        cmap=plt.cm.viridis,\n",
        "                        title='Wavelet Transform (Intensity Spectrum)',\n",
        "                        ylabel='Period [hours]',\n",
        "                        xlabel='Time',\n",
        "                        vlines=False,\n",
        "                        ylim=None):  # Parameter to limit Y axis\n",
        "\n",
        "  dt = time[1] - time[0]\n",
        "  [coefficients, frequencies] = pywt.cwt(signal, scales, waveletname, dt)\n",
        "  power = (abs(coefficients)) ** 2\n",
        "  period = 1.0 / frequencies\n",
        "  levels = [0.1, 0.5, 1, 10, 20, 30, 40, 50, 60, 70, 80, 90]\n",
        "  contourlevels = np.log2(levels)\n",
        "\n",
        "  fig, (ax1, ax2) = plt.subplots(2, 1, figsize=(15, 15), sharex=True, gridspec_kw={'height_ratios': [2, 5]})\n",
        "\n",
        "  # Time Series\n",
        "  ax1.plot(time, signal, label='Series')\n",
        "  ax1.set_title('Time Series', fontsize=15)\n",
        "  ax1.set_ylabel('Series (scaled)', fontsize=12)\n",
        "\n",
        "  # Scalogram\n",
        "  # period * 8760 * 2, because each period value represents 30 minutes in decimal of a year\n",
        "  im = ax2.contourf(time, period * 8760 * 2, np.log2(power), contourlevels, extend='both', cmap=cmap)\n",
        "\n",
        "  # Adjust color scale for better visualization\n",
        "  #im.set_clim(np.min(np.log2(power)), np.max(np.log2(power)))\n",
        "\n",
        "  ax2.set_title(title, fontsize=15)\n",
        "  ax2.set_ylabel(ylabel, fontsize=12)\n",
        "  ax2.set_xlabel(xlabel, fontsize=12)\n",
        "\n",
        "  if ylim is not None:\n",
        "      ax2.set_ylim(ylim)\n",
        "\n",
        "  # Add main vertical lines if necessary\n",
        "  vertical_lines = np.unique(time.astype(int)).tolist()\n",
        "  if vlines:\n",
        "      for line_position in vertical_lines:\n",
        "          ax2.axvline(x=line_position, color='black', linestyle='--', linewidth=1)\n",
        "\n",
        "  cbar_ax = fig.add_axes([0.95, 0.25, 0.03, 0.25]) # [left, bottom, width, height]\n",
        "  fig.colorbar(im, cax=cbar_ax, orientation=\"vertical\", pad=0.1)\n",
        "\n",
        "  plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "def calculate_year_decimal(date):\n",
        "    '''\n",
        "    Converts date series to decimal\n",
        "    '''\n",
        "    year_start = pd.Timestamp(year=date.year, month=1, day=1)\n",
        "    year_end = pd.Timestamp(year=date.year + 1, month=1, day=1)\n",
        "    \n",
        "    # Total number of seconds in the year\n",
        "    year_duration = (year_end - year_start).total_seconds()\n",
        "    \n",
        "    # Number of seconds elapsed since the start of the year\n",
        "    elapsed_time = (date - year_start).total_seconds()\n",
        "    \n",
        "    # Calculate the decimal year\n",
        "    year_decimal = date.year + (elapsed_time / year_duration)\n",
        "    \n",
        "    return year_decimal\n",
        "\n",
        "time = df_series_sales_resampled.index.to_series().apply(calculate_year_decimal).values\n",
        "signal = df_series_sales_resampled.net_value_scld.values\n",
        "\n",
        "plot_signal_plus_average(time, signal)\n",
        "plot_fft_plus_power(time, signal, n=2000)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Available Wavelets\n",
        "(', ').join(pywt.wavelist(kind='continuous'))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "scales = np.arange(1, 300)\n",
        "df_filt_wavelet = df_series_sales_resampled[df_series_sales_resampled.index.year >= 2024]\n",
        "time = df_filt_wavelet.index.to_series().apply(calculate_year_decimal).values\n",
        "signal = df_filt_wavelet.net_value_scld.values\n",
        "plot_wavelet_serie(time, signal, scales, waveletname='morl', vlines=False)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n",
        "#### 3.1.3 Auto Correlations"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Plotting Autocorrelation\n",
        "nlags = (2*24) * 2\n",
        "corr_array = acf(df_series_sales_resampled[['NET_VALUE']], alpha=0.05, nlags=nlags)\n",
        "lower_y = corr_array[1][:,0] - corr_array[0]\n",
        "upper_y = corr_array[1][:,1] - corr_array[0]\n",
        "\n",
        "fig = go.Figure()\n",
        "[fig.add_scatter(x=(x,x), y=(0,corr_array[0][x]), mode='lines',line_color='#3f3f3f') for x in range(len(corr_array[0]))]\n",
        "fig.add_scatter(x=np.arange(len(corr_array[0])), y=corr_array[0], mode='markers', marker_color='#1f77b4', marker_size=6)\n",
        "fig.add_scatter(x=np.arange(len(corr_array[0])), y=upper_y, mode='lines', line_color='rgba(255,255,255,0)')\n",
        "fig.add_scatter(x=np.arange(len(corr_array[0])), y=lower_y, mode='lines',fillcolor='rgba(32, 146, 230,0.3)', fill='tonexty', line_color='rgba(255,255,255,0)')\n",
        "fig.update_traces(showlegend=False)\n",
        "fig.update_xaxes(range=[-1, nlags])\n",
        "fig.update_yaxes(zerolinecolor='#000000')\n",
        "\n",
        "title='Autocorrelation (ACF)'\n",
        "fig.update_layout(title=title,     \n",
        "                  width=1600,\n",
        "                  height=600)\n",
        "fig.show()\n",
        "\n",
        "# Generates the list with lags containing significant correlation\n",
        "indices_out_of_bounds = [i for i, (corr, lower, upper) in enumerate(zip(corr_array[0], lower_y, upper_y)) if corr < lower or corr > upper]\n",
        "print('Lags with significant autocorrelation:')\n",
        "lags_acf = [-x for x in indices_out_of_bounds[1:]]\n",
        "lags_acf.sort()\n",
        "print(lags_acf)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Plotting Partial Autocorrelation\n",
        "corr_array = pacf(df_series_sales_resampled[['NET_VALUE']], alpha=0.05, nlags=nlags)\n",
        "lower_y = corr_array[1][:,0] - corr_array[0]\n",
        "upper_y = corr_array[1][:,1] - corr_array[0]\n",
        "\n",
        "fig = go.Figure()\n",
        "[fig.add_scatter(x=(x,x), y=(0, corr_array[0][x]), mode='lines',line_color='#3f3f3f') for x in range(len(corr_array[0]))]\n",
        "fig.add_scatter(x=np.arange(len(corr_array[0])), y=corr_array[0], mode='markers', marker_color='#1f77b4', marker_size=6)\n",
        "fig.add_scatter(x=np.arange(len(corr_array[0])), y=upper_y, mode='lines', line_color='rgba(255,255,255,0)')\n",
        "fig.add_scatter(x=np.arange(len(corr_array[0])), y=lower_y, mode='lines',fillcolor='rgba(32, 146, 230,0.3)', fill='tonexty', line_color='rgba(255,255,255,0)')\n",
        "fig.update_traces(showlegend=False)\n",
        "fig.update_xaxes(range=[-1, nlags]) \n",
        "fig.update_yaxes(zerolinecolor='#000000')\n",
        "\n",
        "title='Partial Autocorrelation (PACF)'\n",
        "fig.update_layout(title=title,     \n",
        "                  width=1600,\n",
        "                  height=600)\n",
        "fig.show()\n",
        "\n",
        "# Generates the list with lags containing significant correlation\n",
        "indices_out_of_bounds = [i for i, (corr, lower, upper) in enumerate(zip(corr_array[0], lower_y, upper_y)) if corr < lower or corr > upper]\n",
        "print('Lags with significant partial autocorrelation:')\n",
        "lags_pacf = [-x for x in indices_out_of_bounds[1:]]\n",
        "lags_pacf.sort()\n",
        "print(lags_pacf)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n",
        "#### 3.1.4 Date Attributes"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Creating temporal variables\n",
        "df_series_sales_resampled['year'] = df_series_sales_resampled.index.year\n",
        "df_series_sales_resampled['month'] = df_series_sales_resampled.index.month\n",
        "df_series_sales_resampled['week_of_year'] = df_series_sales_resampled.index.isocalendar().week\n",
        "df_series_sales_resampled['day_of_week'] = df_series_sales_resampled.index.day_of_week #  Monday=0 and Sunday=6\n",
        "df_series_sales_resampled['hour'] = df_series_sales_resampled.index.hour\n",
        "\n",
        "# Including day of week name\n",
        "day_week_name = {0: 'Monday', 1: 'Tuesday', 2: 'Wednesday', 3: 'Thursday', 4: 'Friday', 5: 'Saturday', 6: 'Sunday'}\n",
        "df_series_sales_resampled['day_week_name'] = df_series_sales_resampled['day_of_week'].map(day_week_name)\n",
        "\n",
        "# Including holiday\n",
        "brazil_holidays = holidays.Brazil()\n",
        "df_series_sales_resampled['event'] = df_series_sales_resampled.index.map(lambda x: brazil_holidays.get(x, None))\n",
        "\n",
        "# Function to determine custom events\n",
        "def custom_events(date):\n",
        "    month_day = date.strftime('%m-%d')\n",
        "    \n",
        "    # Mother's Day (second Sunday of May)\n",
        "    mothers_day = pd.Timestamp(date.year, 5, 1) + pd.DateOffset(weekday=6, weeks=1)\n",
        "    # Mother's Day Eve\n",
        "    mothers_day_eve = mothers_day - timedelta(days=1)\n",
        "    # Valentines Day (June 12th in Brazil)\n",
        "    valentines_day = pd.Timestamp(date.year, 6, 12)\n",
        "    # Black Friday (fourth Friday of November)\n",
        "    black_friday = pd.Timestamp(date.year, 11, 1) + pd.DateOffset(weekday=4, weeks=3)\n",
        "    \n",
        "    if month_day == mothers_day.strftime('%m-%d'):\n",
        "        return 'Mothers Day'\n",
        "    elif month_day == mothers_day_eve.strftime('%m-%d'):\n",
        "        return 'Mothers Day Eve'\n",
        "    elif month_day == valentines_day.strftime('%m-%d'):\n",
        "        return 'Valentines Day'\n",
        "    elif month_day == black_friday.strftime('%m-%d'):\n",
        "        return 'Black Friday'\n",
        "    else:\n",
        "        return None\n",
        "\n",
        "# Function to include events in holiday column\n",
        "def include_events(row):\n",
        "    if pd.isna(row['event']):\n",
        "        return custom_events(row.name)\n",
        "    else:\n",
        "        return row['event']\n",
        "\n",
        "# Adding custom holidays\n",
        "df_series_sales_resampled['event'] = df_series_sales_resampled.apply(include_events, axis=1)\n",
        "\n",
        "# Creating holiday flag\n",
        "df_series_sales_resampled['event_flag'] = df_series_sales_resampled['event'].apply(lambda x: 1 if x is not None else 0)\n",
        "\n",
        "df_series_sales_resampled['event'].fillna('Normal Day', inplace=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Checking created holidays and events\n",
        "df_series_sales_resampled['event'].unique().tolist()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "df_series_sales_resampled.describe()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "df_series_sales_resampled.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "display(df_series_sales_resampled)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "display(df_series_sales_resampled[(df_series_sales_resampled['event'] == 'Black Friday') | (df_series_sales_resampled['event'].isna())][['NET_VALUE', 'event']].fillna('normal day'))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Verification of existence of statistically significant differences between variables on NET_VALUE\n",
        "categorical_columns = ['event',\n",
        "                        'event_flag',\n",
        "                        'month',\n",
        "                        'week_of_year',\n",
        "                        'day_of_week',\n",
        "                        'hour']\n",
        "\n",
        "# List to store results\n",
        "results = []\n",
        "\n",
        "# Iterate through categorical columns and perform ANOVA test\n",
        "categorical_df = df_series_sales_resampled[categorical_columns + ['NET_VALUE']]\n",
        "categorical_df[categorical_columns] = categorical_df[categorical_columns].astype('category')\n",
        "\n",
        "for col in categorical_columns:\n",
        "    grouped_data = [categorical_df.loc[categorical_df[col] == category, 'NET_VALUE'] for category in categorical_df[col].unique()]\n",
        "    \n",
        "    f_statistic, p_value = f_oneway(*grouped_data)\n",
        "    \n",
        "    significant_difference = \"Yes\" if p_value < 0.05 else \"No\"\n",
        "    \n",
        "    results.append({'Variable': col, 'Significant Difference': significant_difference, 'p-value': p_value})\n",
        "\n",
        "# Create DataFrame with results\n",
        "results_df = pd.DataFrame(results)\n",
        "\n",
        "display(results_df)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Including date variables\n",
        "df_monthly_budget['data'] = pd.to_datetime(df_monthly_budget['month'])\n",
        "df_monthly_budget.drop(columns=['month'], axis=1, inplace=True)\n",
        "df_monthly_budget['year'] = df_monthly_budget.data.dt.year\n",
        "df_monthly_budget['month'] = df_monthly_budget.data.dt.month\n",
        "\n",
        "# Including monthly budget\n",
        "df_series_sales_resampled['data'] = df_series_sales_resampled.index\n",
        "df_series_sales_resampled = df_series_sales_resampled.merge(df_monthly_budget[['year', 'month','total']], on=['year', 'month'], how='left')\n",
        "df_series_sales_resampled.rename(columns={'total': 'month_budget'}, inplace=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "df_series_sales_resampled.describe()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Plotting heatmap with statistical significance annotations\n",
        "df_corr = df_series_sales_resampled.drop(columns=['data', 'event', 'day_week_name', 'zscore'], axis=1).corr()\n",
        "\n",
        "fig, ax = plt.subplots(figsize=(10, 8))\n",
        "sns.heatmap(df_corr,\n",
        "            square = True, \n",
        "            cbar = True, \n",
        "            cmap = 'crest',\n",
        "            ax = ax)\n",
        "\n",
        "# Statistical significance annotations\n",
        "rho = df_series_sales_resampled.drop(columns=['data', 'event', 'day_week_name', 'zscore'], axis=1).corr()\n",
        "pval = df_series_sales_resampled.drop(columns=['data', 'event', 'day_week_name', 'zscore'], axis=1).corr(method=lambda x, y: pearsonr(x, y)[1]) - np.eye(*rho.shape)\n",
        "p = pval.applymap(lambda x: ''.join(['*' for t in [.05, .01, .001] if x<=t]))\n",
        "p = rho.round(2).astype(str) + p\n",
        "\n",
        "for i in range(p.shape[0]):\n",
        "    for j in range(p.shape[1]):\n",
        "        ax.text(j + 0.5, i + 0.5, p.iloc[i, j], ha='center', va='center', fontsize=8)\n",
        "\n",
        "# Adjusting layout\n",
        "plt.title('Correlation Matrix - Pearson')\n",
        "plt.xticks(rotation=90)\n",
        "plt.yticks(rotation=0)\n",
        "plt.tight_layout()\n",
        "\n",
        "# Displaying heatmap\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Plotting heatmap with statistical significance annotations\n",
        "df_corr = df_series_sales_resampled.drop(columns=['event', 'day_week_name', 'zscore'], axis=1).corr(method='spearman')\n",
        "\n",
        "fig, ax = plt.subplots(figsize=(10, 8))\n",
        "sns.heatmap(df_corr,\n",
        "            square = True, \n",
        "            cbar = True, \n",
        "            cmap = 'crest',\n",
        "            ax = ax)\n",
        "\n",
        "# Statistical significance annotations\n",
        "rho = df_series_sales_resampled.drop(columns=['event', 'day_week_name', 'zscore'], axis=1).corr()\n",
        "pval = df_series_sales_resampled.drop(columns=['event', 'day_week_name', 'zscore'], axis=1).corr(method=lambda x, y: spearmanr(x, y)[1]) - np.eye(*rho.shape)\n",
        "p = pval.applymap(lambda x: ''.join(['*' for t in [.05, .01, .001] if x<=t]))\n",
        "p = rho.round(2).astype(str) + p\n",
        "\n",
        "for i in range(p.shape[0]):\n",
        "    for j in range(p.shape[1]):\n",
        "        ax.text(j + 0.5, i + 0.5, p.iloc[i, j], ha='center', va='center', fontsize=8)\n",
        "\n",
        "# Adjusting layout\n",
        "plt.title('Correlation Matrix - Spearman')\n",
        "plt.xticks(rotation=90)\n",
        "plt.yticks(rotation=0)\n",
        "plt.tight_layout()\n",
        "\n",
        "# Displaying heatmap\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n",
        "### 3.2 Daily Cumulative Series"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Creating daily cumulative series\n",
        "df_series_sales_resampled['NET_VALUE_cumulative_day'] = df_series_sales_resampled.groupby(df_series_sales_resampled.data.dt.date)['NET_VALUE'].cumsum()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "display(df_series_sales_resampled.tail(1000))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Plotting Autocorrelation\n",
        "nlags = (2*24) * 2\n",
        "corr_array = acf(df_series_sales_resampled[['NET_VALUE_cumulative_day']], alpha=0.05, nlags=nlags)\n",
        "lower_y = corr_array[1][:,0] - corr_array[0]\n",
        "upper_y = corr_array[1][:,1] - corr_array[0]\n",
        "\n",
        "fig = go.Figure()\n",
        "[fig.add_scatter(x=(x,x), y=(0,corr_array[0][x]), mode='lines',line_color='#3f3f3f') for x in range(len(corr_array[0]))]\n",
        "fig.add_scatter(x=np.arange(len(corr_array[0])), y=corr_array[0], mode='markers', marker_color='#1f77b4', marker_size=6)\n",
        "fig.add_scatter(x=np.arange(len(corr_array[0])), y=upper_y, mode='lines', line_color='rgba(255,255,255,0)')\n",
        "fig.add_scatter(x=np.arange(len(corr_array[0])), y=lower_y, mode='lines',fillcolor='rgba(32, 146, 230,0.3)', fill='tonexty', line_color='rgba(255,255,255,0)')\n",
        "fig.update_traces(showlegend=False)\n",
        "fig.update_xaxes(range=[-1, nlags])\n",
        "fig.update_yaxes(zerolinecolor='#000000')\n",
        "\n",
        "title='Autocorrelation (ACF) - NET_VALUE_cumulative_day'\n",
        "fig.update_layout(title=title,     \n",
        "                  width=1600,\n",
        "                  height=600)\n",
        "fig.show()\n",
        "\n",
        "# Generates the list with lags containing significant correlation\n",
        "indices_out_of_bounds = [i for i, (corr, lower, upper) in enumerate(zip(corr_array[0], lower_y, upper_y)) if corr < lower or corr > upper]\n",
        "print('Lags with significant autocorrelation:')\n",
        "lags_acf = [-x for x in indices_out_of_bounds[1:]]\n",
        "lags_acf.sort()\n",
        "print(lags_acf)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Plotting Partial Autocorrelation\n",
        "corr_array = pacf(df_series_sales_resampled[['NET_VALUE_cumulative_day']], alpha=0.05, nlags=nlags)\n",
        "lower_y = corr_array[1][:,0] - corr_array[0]\n",
        "upper_y = corr_array[1][:,1] - corr_array[0]\n",
        "\n",
        "fig = go.Figure()\n",
        "[fig.add_scatter(x=(x,x), y=(0, corr_array[0][x]), mode='lines',line_color='#3f3f3f') for x in range(len(corr_array[0]))]\n",
        "fig.add_scatter(x=np.arange(len(corr_array[0])), y=corr_array[0], mode='markers', marker_color='#1f77b4', marker_size=6)\n",
        "fig.add_scatter(x=np.arange(len(corr_array[0])), y=upper_y, mode='lines', line_color='rgba(255,255,255,0)')\n",
        "fig.add_scatter(x=np.arange(len(corr_array[0])), y=lower_y, mode='lines',fillcolor='rgba(32, 146, 230,0.3)', fill='tonexty', line_color='rgba(255,255,255,0)')\n",
        "fig.update_traces(showlegend=False)\n",
        "fig.update_xaxes(range=[-1, nlags]) \n",
        "fig.update_yaxes(zerolinecolor='#000000')\n",
        "\n",
        "title='Partial Autocorrelation (PACF) - NET_VALUE_cumulative_day'\n",
        "fig.update_layout(title=title,     \n",
        "                  width=1600,\n",
        "                  height=600)\n",
        "fig.show()\n",
        "\n",
        "# Generates the list with lags containing significant correlation\n",
        "indices_out_of_bounds = [i for i, (corr, lower, upper) in enumerate(zip(corr_array[0], lower_y, upper_y)) if corr < lower or corr > upper]\n",
        "print('Lags with significant partial autocorrelation:')\n",
        "lags_pacf = [-x for x in indices_out_of_bounds[1:]]\n",
        "lags_pacf.sort()\n",
        "print(lags_pacf)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Plotting heatmap with statistical significance annotations\n",
        "df_corr = df_series_sales_resampled.drop(columns=['data', 'event', 'day_week_name', 'zscore'], axis=1).corr()\n",
        "\n",
        "fig, ax = plt.subplots(figsize=(10, 8))\n",
        "sns.heatmap(df_corr,\n",
        "            square = True, \n",
        "            cbar = True, \n",
        "            cmap = 'crest',\n",
        "            ax = ax)\n",
        "\n",
        "# Statistical significance annotations\n",
        "rho = df_series_sales_resampled.drop(columns=['data', 'event', 'day_week_name', 'zscore'], axis=1).corr()\n",
        "pval = df_series_sales_resampled.drop(columns=['data', 'event', 'day_week_name', 'zscore'], axis=1).corr(method=lambda x, y: pearsonr(x, y)[1]) - np.eye(*rho.shape)\n",
        "p = pval.applymap(lambda x: ''.join(['*' for t in [.05, .01, .001] if x<=t]))\n",
        "p = rho.round(2).astype(str) + p\n",
        "\n",
        "for i in range(p.shape[0]):\n",
        "    for j in range(p.shape[1]):\n",
        "        ax.text(j + 0.5, i + 0.5, p.iloc[i, j], ha='center', va='center', fontsize=8)\n",
        "\n",
        "# Adjusting layout\n",
        "plt.title('Correlation Matrix - Pearson')\n",
        "plt.xticks(rotation=90)\n",
        "plt.yticks(rotation=0)\n",
        "plt.tight_layout()\n",
        "\n",
        "# Displaying heatmap\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Plotting heatmap with statistical significance annotations\n",
        "df_corr = df_series_sales_resampled.drop(columns=['data', 'event', 'day_week_name', 'zscore'], axis=1).corr(method='spearman')\n",
        "\n",
        "fig, ax = plt.subplots(figsize=(10, 8))\n",
        "sns.heatmap(df_corr,\n",
        "            square = True, \n",
        "            cbar = True, \n",
        "            cmap = 'crest',\n",
        "            ax = ax)\n",
        "\n",
        "# Statistical significance annotations\n",
        "rho = df_series_sales_resampled.drop(columns=['data', 'event', 'day_week_name', 'zscore'], axis=1).corr()\n",
        "pval = df_series_sales_resampled.drop(columns=['data', 'event', 'day_week_name', 'zscore'], axis=1).corr(method=lambda x, y: spearmanr(x, y)[1]) - np.eye(*rho.shape)\n",
        "p = pval.applymap(lambda x: ''.join(['*' for t in [.05, .01, .001] if x<=t]))\n",
        "p = rho.round(2).astype(str) + p\n",
        "\n",
        "for i in range(p.shape[0]):\n",
        "    for j in range(p.shape[1]):\n",
        "        ax.text(j + 0.5, i + 0.5, p.iloc[i, j], ha='center', va='center', fontsize=8)\n",
        "\n",
        "# Adjusting layout\n",
        "plt.title('Correlation Matrix - Spearman')\n",
        "plt.xticks(rotation=90)\n",
        "plt.yticks(rotation=0)\n",
        "plt.tight_layout()\n",
        "\n",
        "# Displaying heatmap\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n",
        "\n",
        "### 3.3 3-Hour Granularity Series"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Setting SYSTEM_TIMESTAMP column as DataFrame index\n",
        "df_series_sales.set_index('SYSTEM_TIMESTAMP', inplace=True)\n",
        "df_series_sales.sort_index(inplace=True)\n",
        "\n",
        "# Converting granularity to 3 hours\n",
        "df_series_sales_resampled_3h = df_series_sales.resample('3H').sum()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "display(df_series_sales_resampled_3h.reset_index().iloc[-100:])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Plotting Autocorrelation\n",
        "nlags = (8*24) * 2\n",
        "corr_array = acf(df_series_sales_resampled_3h[['NET_VALUE']], alpha=0.05, nlags=nlags)\n",
        "lower_y = corr_array[1][:,0] - corr_array[0]\n",
        "upper_y = corr_array[1][:,1] - corr_array[0]\n",
        "\n",
        "fig = go.Figure()\n",
        "[fig.add_scatter(x=(x,x), y=(0,corr_array[0][x]), mode='lines',line_color='#3f3f3f') for x in range(len(corr_array[0]))]\n",
        "fig.add_scatter(x=np.arange(len(corr_array[0])), y=corr_array[0], mode='markers', marker_color='#1f77b4', marker_size=6)\n",
        "fig.add_scatter(x=np.arange(len(corr_array[0])), y=upper_y, mode='lines', line_color='rgba(255,255,255,0)')\n",
        "fig.add_scatter(x=np.arange(len(corr_array[0])), y=lower_y, mode='lines',fillcolor='rgba(32, 146, 230,0.3)', fill='tonexty', line_color='rgba(255,255,255,0)')\n",
        "fig.update_traces(showlegend=False)\n",
        "fig.update_xaxes(range=[-1, nlags])\n",
        "fig.update_yaxes(zerolinecolor='#000000')\n",
        "\n",
        "title='Autocorrelation (ACF)'\n",
        "fig.update_layout(title=title,     \n",
        "                  width=1600,\n",
        "                  height=600)\n",
        "fig.show()\n",
        "\n",
        "# Generates the list with lags containing significant correlation\n",
        "indices_out_of_bounds = [i for i, (corr, lower, upper) in enumerate(zip(corr_array[0], lower_y, upper_y)) if corr < lower or corr > upper]\n",
        "print('Lags with significant autocorrelation:')\n",
        "lags_acf = [-x for x in indices_out_of_bounds[1:]]\n",
        "lags_acf.sort()\n",
        "print(lags_acf)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n",
        "### 3.4 Daily Series"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Setting SYSTEM_TIMESTAMP column as DataFrame index\n",
        "df_series_sales.set_index('SYSTEM_TIMESTAMP', inplace=True)\n",
        "df_series_sales.sort_index(inplace=True)\n",
        "\n",
        "# Converting granularity to Daily\n",
        "df_series_sales_resampled_dia = df_series_sales.resample('D').sum()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "display(df_series_sales_resampled_dia.reset_index())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Plotting Autocorrelation\n",
        "nlags = 180\n",
        "corr_array = acf(df_series_sales_resampled_dia[['NET_VALUE']], alpha=0.05, nlags=nlags)\n",
        "lower_y = corr_array[1][:,0] - corr_array[0]\n",
        "upper_y = corr_array[1][:,1] - corr_array[0]\n",
        "\n",
        "fig = go.Figure()\n",
        "[fig.add_scatter(x=(x,x), y=(0,corr_array[0][x]), mode='lines',line_color='#3f3f3f') for x in range(len(corr_array[0]))]\n",
        "fig.add_scatter(x=np.arange(len(corr_array[0])), y=corr_array[0], mode='markers', marker_color='#1f77b4', marker_size=6)\n",
        "fig.add_scatter(x=np.arange(len(corr_array[0])), y=upper_y, mode='lines', line_color='rgba(255,255,255,0)')\n",
        "fig.add_scatter(x=np.arange(len(corr_array[0])), y=lower_y, mode='lines',fillcolor='rgba(32, 146, 230,0.3)', fill='tonexty', line_color='rgba(255,255,255,0)')\n",
        "fig.update_traces(showlegend=False)\n",
        "fig.update_xaxes(range=[-1, nlags])\n",
        "fig.update_yaxes(zerolinecolor='#000000')\n",
        "\n",
        "title='Autocorrelation (ACF)'\n",
        "fig.update_layout(title=title,     \n",
        "                  width=1600,\n",
        "                  height=600)\n",
        "fig.show()\n",
        "\n",
        "# Generates the list with lags containing significant correlation\n",
        "indices_out_of_bounds = [i for i, (corr, lower, upper) in enumerate(zip(corr_array[0], lower_y, upper_y)) if corr < lower or corr > upper]\n",
        "print('Lags with significant autocorrelation:')\n",
        "lags_acf = [-x for x in indices_out_of_bounds[1:]]\n",
        "lags_acf.sort()\n",
        "print(lags_acf)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Plotting Partial Autocorrelation\n",
        "corr_array = pacf(df_series_sales_resampled_dia[['NET_VALUE']], alpha=0.05, nlags=nlags)\n",
        "lower_y = corr_array[1][:,0] - corr_array[0]\n",
        "upper_y = corr_array[1][:,1] - corr_array[0]\n",
        "\n",
        "fig = go.Figure()\n",
        "[fig.add_scatter(x=(x,x), y=(0, corr_array[0][x]), mode='lines',line_color='#3f3f3f') for x in range(len(corr_array[0]))]\n",
        "fig.add_scatter(x=np.arange(len(corr_array[0])), y=corr_array[0], mode='markers', marker_color='#1f77b4', marker_size=6)\n",
        "fig.add_scatter(x=np.arange(len(corr_array[0])), y=upper_y, mode='lines', line_color='rgba(255,255,255,0)')\n",
        "fig.add_scatter(x=np.arange(len(corr_array[0])), y=lower_y, mode='lines',fillcolor='rgba(32, 146, 230,0.3)', fill='tonexty', line_color='rgba(255,255,255,0)')\n",
        "fig.update_traces(showlegend=False)\n",
        "fig.update_xaxes(range=[-1, nlags]) \n",
        "fig.update_yaxes(zerolinecolor='#000000')\n",
        "\n",
        "title='Partial Autocorrelation (PACF)'\n",
        "fig.update_layout(title=title,     \n",
        "                  width=1600,\n",
        "                  height=600)\n",
        "fig.show()\n",
        "\n",
        "# Generates the list with lags containing significant correlation\n",
        "indices_out_of_bounds = [i for i, (corr, lower, upper) in enumerate(zip(corr_array[0], lower_y, upper_y)) if corr < lower or corr > upper]\n",
        "print('Lags with significant partial autocorrelation:')\n",
        "lags_pacf = [-x for x in indices_out_of_bounds[1:]]\n",
        "lags_pacf.sort()\n",
        "print(lags_pacf)"
      ],
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 4
}