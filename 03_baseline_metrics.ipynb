{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Databricks notebook source"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Darts\n",
        "from darts import TimeSeries\n",
        "from darts.models.forecasting.lgbm import LightGBMModel\n",
        "from darts import TimeSeries\n",
        "from darts import metrics\n",
        "from darts.dataprocessing.transformers import Scaler\n",
        "\n",
        "# Analysis\n",
        "from statsmodels.tsa.seasonal import seasonal_decompose\n",
        "from statsmodels.stats.diagnostic import acorr_ljungbox\n",
        "from statsmodels.graphics.tsaplots import plot_acf, plot_pacf\n",
        "\n",
        "# Plotting\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "# Utilities\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import datetime\n",
        "import mlflow\n",
        "import mlflow.sklearn\n",
        "from numpy import savetxt\n",
        "from datetime import timedelta\n",
        "import holidays\n",
        "from scipy import stats\n",
        "from sklearn.preprocessing import MinMaxScaler, PowerTransformer\n",
        "\n",
        "# Notebook configuration\n",
        "%matplotlib inline\n",
        "%config InlineBackend.figure_format = 'retina'\n",
        "sns.set_theme(style=\"whitegrid\", palette=\"pastel\")\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Importing sales series\n",
        "str_select_sales = '''\n",
        "                        SELECT *\n",
        "                        FROM analytics.refined_sales_orders_agg\n",
        "                        '''\n",
        "\n",
        "df_series_sales = spark.sql(str_select_sales).toPandas()\n",
        "\n",
        "df_series_sales.info()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Sets SYSTEM_TIMESTAMP column as DataFrame index\n",
        "df_series_sales.set_index('SYSTEM_TIMESTAMP', inplace=True)\n",
        "df_series_sales.sort_index(inplace=True)\n",
        "\n",
        "# Converts granularity to 30 minutes\n",
        "df_series_sales_resampled = df_series_sales.resample('30T').sum()\n",
        "\n",
        "# Creates a date range with 30-minute granularity between start and end date\n",
        "date_range = pd.date_range(start=df_series_sales.index.min().strftime('%Y-%m-%d %H:00:00'), end=df_series_sales.index.max().strftime('%Y-%m-%d %H:30:00'), freq='30T')\n",
        "\n",
        "# Checking if original data has all time points - must be 0\n",
        "date_range.difference(df_series_sales_resampled.index).shape[0]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "df_series_sales_resampled.info()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Function to calculate hourly representations for historical weekdays\n",
        "def add_pct_columns(df, method='mean', years=[2024], months=[2, 3, 4], col='NET_VALUE_clean'):\n",
        "    # Add columns for day of week and time of day\n",
        "    df['day_of_week'] = df.index.dayofweek\n",
        "    df['time_of_day'] = df.index.time\n",
        "    \n",
        "    # Calculate current percentage representation\n",
        "    df['day_total'] = df.groupby(df.index.date)[col].transform('sum')\n",
        "    df['pct_current'] = df[col] / df['day_total']\n",
        "    \n",
        "    # Function to calculate desired statistic (mean or median)\n",
        "    def calc_stat(group, method):\n",
        "        if method == 'mean':\n",
        "            return group.mean()\n",
        "        elif method == 'median':\n",
        "            return group.median()\n",
        "        else:\n",
        "            raise ValueError(\"Method must be 'mean' or 'median'\")\n",
        "    \n",
        "    # Calculate historical mean or median for each day of week and time of day combination\n",
        "    # Filters specific months to follow the baseline\n",
        "    history_df = df[(df.index.year.isin(years)) & (df.index.month.isin(months))]\n",
        "    \n",
        "    pct_hist = history_df.groupby(['day_of_week', 'time_of_day'])['pct_current'].apply(calc_stat, method=method).reset_index()\n",
        "    pct_hist.columns = ['day_of_week', 'time_of_day', 'pct_hist']\n",
        "    \n",
        "    # Merge historical statistics to original dataframe\n",
        "    df = df.merge(pct_hist, on=['day_of_week', 'time_of_day'], how='left')\n",
        "    \n",
        "    return df, history_df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Removing outliers\n",
        "df_series_sales_resampled['zscore'] = stats.zscore(df_series_sales_resampled[['NET_VALUE']])\n",
        "outliers_index = df_series_sales_resampled[df_series_sales_resampled.zscore.abs() >= 4].index\n",
        "df_series_sales_resampled['NET_VALUE_clean'] = df_series_sales_resampled['NET_VALUE']\n",
        "df_series_sales_resampled.loc[outliers_index, 'NET_VALUE_clean'] = np.nan\n",
        "df_series_sales_resampled['NET_VALUE_clean'] = df_series_sales_resampled['NET_VALUE_clean'].interpolate()\n",
        "\n",
        "# Generating calculated data\n",
        "df_calculation, history_df = add_pct_columns(df_series_sales_resampled, method='mean', col='NET_VALUE_clean')\n",
        "df_calculation['SYSTEM_TIMESTAMP'] = df_series_sales_resampled.index\n",
        "\n",
        "# Adjusting date column for visualization using display()\n",
        "df_calculation['time_of_day'] = pd.to_datetime(df_calculation['time_of_day'], format='%H:%M:00').dt.time\n",
        "df_calculation['datetime'] = df_calculation.apply(lambda row: pd.Timestamp.combine(row['SYSTEM_TIMESTAMP'], row['time_of_day']), axis=1)\n",
        "df_calculation = df_calculation.drop(columns=['SYSTEM_TIMESTAMP'])\n",
        "df_calculation.rename(columns={'datetime': 'SYSTEM_TIMESTAMP'}, inplace=True)\n",
        "\n",
        "# Filtering only the specific hours checked by the analyst\n",
        "df_calculation_filt = df_calculation[df_calculation.time_of_day.isin([datetime.time(12, 0), datetime.time(15, 0), datetime.time(17, 30), datetime.time(23, 30)])].copy()\n",
        "df_calculation = df_calculation.drop(columns=['time_of_day'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "display(df_calculation[df_calculation.SYSTEM_TIMESTAMP >= '2024-05-01 00:00:00'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Generating series for test period\n",
        "dt_test = '2023-12-18 00:00:00'\n",
        "df_test = df_calculation[df_calculation.SYSTEM_TIMESTAMP >= dt_test].copy()\n",
        "\n",
        "series_real = TimeSeries.from_dataframe(df_test,\n",
        "                                       time_col='SYSTEM_TIMESTAMP',\n",
        "                                       value_cols='pct_current',\n",
        "                                       freq='30T')\n",
        "\n",
        "series_pred = TimeSeries.from_dataframe(df_test,\n",
        "                                       time_col='SYSTEM_TIMESTAMP',\n",
        "                                       value_cols='pct_hist',\n",
        "                                       freq='30T')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Factor to add to series to be measured, avoiding zeros\n",
        "# Following the same proportion as the factor used in model metrics\n",
        "factor = 1 / df_test.NET_VALUE_clean.max()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Calculating metrics\n",
        "\n",
        "def calculate_wape(y, yhat):\n",
        "    \"\"\"\n",
        "    Calculates Weighted Absolute Percentage Error (WAPE).\n",
        "    \n",
        "    Args:\n",
        "        y (list or numpy array): Actual values.\n",
        "        yhat (list or numpy array): Predicted values.\n",
        "    \n",
        "    Returns:\n",
        "        float: WAPE value.\n",
        "        \n",
        "    Description:\n",
        "    This function calculates the Weighted Absolute Percentage Error (WAPE).\n",
        "    \n",
        "    WAPE takes into account both absolute error and percentage error between actual and predicted values. \n",
        "    Furthermore, it performs error weighting based on actual values. In seasonal sales time series scenarios, \n",
        "    seasonal peak periods are assigned a higher weight, so errors in these periods have a more significant impact on this metric.\n",
        "    \n",
        "    Weighting is performed by assigning higher weights to larger actual values, since errors in high demand periods \n",
        "    can have a more relevant impact on planning decisions and financial results.\n",
        "    \n",
        "    In the code section where weight is defined as 'weight = actual', it is considered that the weight of each observation \n",
        "    is equal to the actual value of that observation itself. This means that when calculating WAPE, absolute percentage errors \n",
        "    are multiplied by the actual values themselves before being summed to calculate total weighted error.\n",
        "    \"\"\"\n",
        "    \n",
        "    # Initializes variables to store total WAPE and total weights\n",
        "    total_wape = 0\n",
        "    total_weight = 0\n",
        "    \n",
        "    try:\n",
        "        # Loop over actual (y) and predicted (yhat) values\n",
        "        for i in range(len(y)):\n",
        "            actual = y[i]  # Actual value\n",
        "            predicted = yhat[i]  # Predicted value\n",
        "            \n",
        "            # Calculates absolute error between actual and predicted value\n",
        "            absolute_error = abs(actual - predicted)\n",
        "            \n",
        "            # Calculates absolute percentage error\n",
        "            absolute_percentage_error = absolute_error / actual\n",
        "            \n",
        "            # Calculates weight as the actual value (used for weighting)\n",
        "            weight = actual\n",
        "            \n",
        "            # Updates total WAPE summing weighted percentage error\n",
        "            total_wape += absolute_percentage_error * weight\n",
        "            \n",
        "            # Updates total weights\n",
        "            total_weight += weight\n",
        "        \n",
        "        # Calculates final WAPE as weighted average of percentage errors\n",
        "        wape = total_wape / total_weight * 100\n",
        "    except:\n",
        "        # If an exception occurs (e.g., division by zero), sets WAPE as infinite\n",
        "        wape = np.inf\n",
        "        \n",
        "    return wape\n",
        "\n",
        "mape = metrics.mape(series_real + factor, series_pred + factor)\n",
        "smape = metrics.smape(series_real + factor, series_pred + factor)\n",
        "ope = metrics.ope(series_real + factor, series_pred + factor)\n",
        "r2 = metrics.r2_score(series_real + factor, series_pred + factor)\n",
        "rmse = metrics.rmse(series_real + factor, series_pred + factor)\n",
        "wape = calculate_wape(series_real + factor, series_pred + factor).values().flatten()[0]\n",
        "\n",
        "print('WAPE: ', round(wape, 2))\n",
        "print('R²: ', round(r2, 2))\n",
        "print('\\n')\n",
        "print('MAPE: ', round(mape, 2))\n",
        "print('OPE: ', round(ope, 2))\n",
        "print('RMSE: ', round(rmse, 2))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "ope"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "plt.figure(figsize=(20, 7))\n",
        "series_pred[-80:].stack(series_real[-80:]).plot()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Cumulative metrics\n",
        "df_calculation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "df_calculation_acc = df_calculation.groupby(df_calculation.SYSTEM_TIMESTAMP.dt.date)[['pct_hist', 'pct_current']].cumsum()\n",
        "df_calculation_acc['SYSTEM_TIMESTAMP'] = df_calculation['SYSTEM_TIMESTAMP']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "display(df_calculation_acc.iloc[-1000:])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Generating series for test period\n",
        "dt_test = '2023-12-18 00:00:00'\n",
        "df_test = df_calculation_acc[df_calculation_acc.SYSTEM_TIMESTAMP >= dt_test].copy()\n",
        "\n",
        "series_real = TimeSeries.from_dataframe(df_test,\n",
        "                                       time_col='SYSTEM_TIMESTAMP',\n",
        "                                       value_cols='pct_current',\n",
        "                                       freq='30T')\n",
        "\n",
        "series_pred = TimeSeries.from_dataframe(df_test,\n",
        "                                       time_col='SYSTEM_TIMESTAMP',\n",
        "                                       value_cols='pct_hist',\n",
        "                                       freq='30T')\n",
        "\n",
        "mape = metrics.mape(series_real + factor, series_pred + factor)\n",
        "smape = metrics.smape(series_real + factor, series_pred + factor)\n",
        "ope = metrics.ope(series_real + factor, series_pred + factor)\n",
        "r2 = metrics.r2_score(series_real + factor, series_pred + factor)\n",
        "rmse = metrics.rmse(series_real + factor, series_pred + factor)\n",
        "wape = calculate_wape(series_real + factor, series_pred + factor).values().flatten()[0]\n",
        "\n",
        "print('WAPE: ', round(wape, 2))\n",
        "print('R²: ', round(r2, 2))\n",
        "print('\\n')\n",
        "print('MAPE: ', round(mape, 2))\n",
        "print('OPE: ', round(ope, 2))\n",
        "print('RMSE: ', round(rmse, 2))"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 4
}
