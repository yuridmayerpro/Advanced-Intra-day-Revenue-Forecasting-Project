{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Databricks notebook source\n",
        "\n",
        "## 1. Libraries"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "!pip install u8darts[all]==0.29.0\n",
        "!pip install mlflow==2.11.3\n",
        "!pip install holidays==0.45\n",
        "!pip install numpy==1.23.5\n",
        "dbutils.library.restartPython()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Darts\n",
        "from darts import TimeSeries\n",
        "from darts.models.forecasting.lgbm import LightGBMModel\n",
        "from darts.dataprocessing.transformers import Scaler\n",
        "from darts import metrics\n",
        "\n",
        "# Plotting\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import plotly.express as px\n",
        "\n",
        "# Utilities\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import datetime\n",
        "import mlflow\n",
        "import mlflow.sklearn\n",
        "from datetime import timedelta\n",
        "import datetime\n",
        "import holidays\n",
        "from scipy import stats\n",
        "from sklearn.preprocessing import PowerTransformer, MinMaxScaler\n",
        "from dateutil.relativedelta import relativedelta\n",
        "from tqdm import tqdm\n",
        "import pickle\n",
        "from statsmodels.tsa.seasonal import seasonal_decompose\n",
        "\n",
        "# Notebook configuration\n",
        "%matplotlib inline\n",
        "%config InlineBackend.figure_format = 'retina'\n",
        "sns.set_theme(style=\"whitegrid\", palette=\"pastel\")\n",
        "#import warnings\n",
        "#warnings.filterwarnings('ignore')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n",
        "## 2. Utility Functions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Function to calculate Easter date\n",
        "def calculate_easter(year):\n",
        "    \"Returns the Easter date for a given year.\"\n",
        "    a = year % 19\n",
        "    b = year // 100\n",
        "    c = year % 100\n",
        "    d = b // 4\n",
        "    e = b % 4\n",
        "    f = (b + 8) // 25\n",
        "    g = (b - f + 1) // 3\n",
        "    h = (19 * a + b - d - g + 15) % 30\n",
        "    i = c // 4\n",
        "    k = c % 4\n",
        "    l = (32 + 2 * e + 2 * i - h - k) % 7\n",
        "    m = (a + 11 * h + 22 * l) // 451\n",
        "    month = (h + l - 7 * m + 114) // 31\n",
        "    day = ((h + l - 7 * m + 114) % 31) + 1\n",
        "    return pd.Timestamp(year, month, day)\n",
        "\n",
        "\n",
        "# Function to calculate Carnival date\n",
        "def calculate_carnival(year):\n",
        "    \"Returns the Carnival date for a given year.\"\n",
        "    easter = calculate_easter(year)\n",
        "    return easter - timedelta(days=47)\n",
        "    \n",
        "\n",
        "# Function to determine custom events\n",
        "def custom_events(date):\n",
        "    month_day = date.strftime('%m-%d')\n",
        "    year = date.year\n",
        "\n",
        "    # Carnival\n",
        "    carnival = calculate_carnival(year)\n",
        "    # Carnival Eve\n",
        "    carnival_eve = carnival - timedelta(days=1)\n",
        "    # Mother's Day (second Sunday of May)\n",
        "    mothers_day = pd.Timestamp(year, 5, 1) + pd.DateOffset(weekday=6, weeks=1)\n",
        "    # Mother's Day Eve\n",
        "    mothers_day_eve = mothers_day - timedelta(days=1)\n",
        "    # Valentines Day (June 12th in Brazil)\n",
        "    valentines_day = pd.Timestamp(year, 6, 12)\n",
        "    # Black Friday (fourth Friday of November)\n",
        "    black_friday = pd.Timestamp(year, 11, 1) + pd.DateOffset(weekday=4, weeks=3)\n",
        "    \n",
        "    if date == carnival:\n",
        "        return 'Carnival'\n",
        "    elif date == carnival_eve:\n",
        "        return 'Carnival Eve'\n",
        "    elif month_day == mothers_day.strftime('%m-%d'):\n",
        "        return 'Mothers Day'\n",
        "    elif month_day == mothers_day_eve.strftime('%m-%d'):\n",
        "        return 'Mothers Day Eve'\n",
        "    elif month_day == valentines_day.strftime('%m-%d'):\n",
        "        return 'Valentines Day'\n",
        "    elif month_day == black_friday.strftime('%m-%d'):\n",
        "        return 'Black Friday'\n",
        "    else:\n",
        "        return None\n",
        "\n",
        "\n",
        "# Function to include events in holiday column\n",
        "def include_events(row):\n",
        "    if pd.isna(row['event']):\n",
        "        return custom_events(row.name)\n",
        "    else:\n",
        "        return row['event']\n",
        "    \n",
        "\n",
        "# Function to replace values\n",
        "def replace_event(row):\n",
        "    if row['event'] not in events_to_consider:\n",
        "        return 'Normal Day'\n",
        "    else:\n",
        "        return row['event']\n",
        "\n",
        "def replace_or_add(lst, index, new_value):\n",
        "    if index < len(lst):\n",
        "        # If index exists in list, replace item\n",
        "        lst[index] = new_value\n",
        "    else:\n",
        "        # If index does not exist, append new value to end of list\n",
        "        lst.append(new_value)\n",
        "\n",
        "\n",
        "# Function to remove outliers from metrics\n",
        "def remove_outliers_zscore(data_list, threshold=3):\n",
        "    # Remove null values\n",
        "    data_np = np.array([x for x in data_list if x is not np.nan])\n",
        "\n",
        "    if len(data_np) == 0:\n",
        "        return []\n",
        "\n",
        "    # Calculate z-scores and filter outliers\n",
        "    z_scores = np.abs((data_np - np.mean(data_np)) / np.std(data_np))\n",
        "    filtered_data = data_np[z_scores < threshold]\n",
        "\n",
        "    return filtered_data.tolist()\n",
        "\n",
        "    \n",
        "def prepare_input(df, \n",
        "                  target_col='NET_VALUE',\n",
        "                  numeric_features=None, \n",
        "                  use_event_flag=False, \n",
        "                  events_to_consider=None,\n",
        "                  scale_trgt=False,\n",
        "                  scale_cov=False,\n",
        "                  scale_method=None,\n",
        "                  dummy_day_of_week=False):\n",
        "    '''\n",
        "    Function to prepare model input data.\n",
        "\n",
        "    Parameters:\n",
        "    - df (pandas.DataFrame): DataFrame containing the data used.\n",
        "    - target_col (str): Name of the target series column.\n",
        "    - numeric_features (list): List with numerical feature columns to be used.\n",
        "    - use_event_flag (boolean): Whether to use the event flag.\n",
        "    - events_to_consider (list): List with event names to consider if used as variables.\n",
        "    - scale_trgt (boolean): Whether to scale the target variable.\n",
        "    - scale_cov (boolean): Whether to scale the covariates.\n",
        "    - scale_method (str): Name of the method to be used when scaling data (minmax, power). \n",
        "\n",
        "    Returns:\n",
        "    - list with the following items:\n",
        "        - series (TimeSeries (DataArray)): target series.\n",
        "        - series_cov (TimeSeries (DataArray)): covariate series.\n",
        "        - transformer_trgt (darts.dataprocessing.transformers.scaler.Scaler): target series scaler for scale reversal.\n",
        "    '''\n",
        "    return_list = []\n",
        "\n",
        "    # Generates main series\n",
        "    series = TimeSeries.from_dataframe(\n",
        "                                        df=df,\n",
        "                                        value_cols=target_col,\n",
        "                                        freq='D')\n",
        "    return_list.append(series)\n",
        "\n",
        "    # Initialize series_cov\n",
        "    series_cov = None\n",
        "\n",
        "    # Creates covariate series for numerical variables\n",
        "    if numeric_features:\n",
        "        series_cov =  TimeSeries.from_dataframe(\n",
        "                                                df=df,\n",
        "                                                value_cols=numeric_features,\n",
        "                                                freq='D')\n",
        "        replace_or_add(return_list, 1, series_cov)\n",
        "\n",
        "    # Creates series for event_flag variable\n",
        "    if use_event_flag:\n",
        "        series_event_flag = TimeSeries.from_dataframe(\n",
        "                                                        df=df,\n",
        "                                                        value_cols='event_flag',\n",
        "                                                        freq='D')\n",
        "        \n",
        "        series_cov = series_cov.stack(series_event_flag) if series_cov else series_event_flag\n",
        "        replace_or_add(return_list, 1, series_cov)\n",
        "\n",
        "    # Creates series with dummy of events to consider\n",
        "    if events_to_consider:\n",
        "        df['event'] = df.apply(replace_event, axis=1)\n",
        "        df_dummy = pd.get_dummies(df['event'], prefix='event') * 1\n",
        "        series_dummy = TimeSeries.from_dataframe(\n",
        "                                                    df=df_dummy,\n",
        "                                                    value_cols=df_dummy.columns.tolist(),\n",
        "                                                    freq='D')\n",
        "        \n",
        "        series_cov = series_cov.stack(series_dummy) if series_cov else series_dummy\n",
        "        replace_or_add(return_list, 1, series_cov)\n",
        "\n",
        "    # Creates series with day of week dummy\n",
        "    if dummy_day_of_week:\n",
        "        df_dummy_dayofweek = pd.get_dummies(df['day_of_week'], prefix='day_of_week') * 1\n",
        "        series_dummy_dayofweek = TimeSeries.from_dataframe(\n",
        "                                                            df=df_dummy_dayofweek,\n",
        "                                                            value_cols=df_dummy_dayofweek.columns.tolist(),\n",
        "                                                            freq='D')\n",
        "        series_cov = series_cov.stack(series_dummy_dayofweek) if series_cov else series_dummy_dayofweek\n",
        "        replace_or_add(return_list, 1, series_cov)\n",
        "\n",
        "    # Initialize transformers\n",
        "    transformer_trgt = None\n",
        "    transformer_cov = None\n",
        "\n",
        "    # Performs scaling if desired\n",
        "    if scale_trgt:\n",
        "        if scale_method == 'minmax':\n",
        "            scaler_trgt = MinMaxScaler()\n",
        "        else:\n",
        "            scaler_trgt = PowerTransformer()\n",
        "            \n",
        "        transformer_trgt = Scaler(scaler_trgt)\n",
        "        series = transformer_trgt.fit_transform(series)\n",
        "        replace_or_add(return_list, 0, series)\n",
        "        replace_or_add(return_list, 2, transformer_trgt)\n",
        "\n",
        "    if scale_cov and series_cov:\n",
        "        if scale_method == 'minmax':\n",
        "            scaler_cov = MinMaxScaler()\n",
        "        else:\n",
        "            scaler_cov = PowerTransformer()\n",
        "            \n",
        "        transformer_cov = Scaler(scaler_cov)\n",
        "        series_cov = transformer_cov.fit_transform(series_cov)\n",
        "        replace_or_add(return_list, 1, series_cov)\n",
        "\n",
        "\n",
        "    return return_list\n",
        "\n",
        "\n",
        "# Function to create the model\n",
        "def create_model(use_covariates, lags, output_chunk_length, random_seed, n_estimators, multi_models, lags_past_covariates, lags_future_covariates):\n",
        "    if use_covariates:\n",
        "        return LightGBMModel(\n",
        "            lags=lags,\n",
        "            output_chunk_length=output_chunk_length,\n",
        "            lags_past_covariates=lags_past_covariates,\n",
        "            lags_future_covariates=lags_future_covariates,\n",
        "            random_state=random_seed,\n",
        "            n_estimators=n_estimators,\n",
        "            multi_models=multi_models,\n",
        "            force_row_wise=False,\n",
        "            force_col_wise=False)\n",
        "    else:\n",
        "        return LightGBMModel(\n",
        "            lags=lags,\n",
        "            output_chunk_length=output_chunk_length,\n",
        "            random_state=random_seed,\n",
        "            n_estimators=n_estimators,\n",
        "            multi_models=multi_models,\n",
        "            force_row_wise=False,\n",
        "            force_col_wise=True)\n",
        "\n",
        "\n",
        "# Transform negative values to zero\n",
        "def replace_negatives_with_zero(x):\n",
        "    return np.maximum(x, 0)\n",
        "\n",
        "\n",
        "# Function to calculate WAPE\n",
        "def calculate_wape(y, yhat):\n",
        "    \"\"\"\n",
        "    Calculates Weighted Absolute Percentage Error (WAPE).\n",
        "    \n",
        "    Args:\n",
        "        y (list or numpy array): Actual values.\n",
        "        yhat (list or numpy array): Predicted values.\n",
        "    \n",
        "    Returns:\n",
        "        float: WAPE value.\n",
        "        \n",
        "    Description:\n",
        "    This function calculates the Weighted Absolute Percentage Error (WAPE).\n",
        "    \n",
        "    WAPE takes into account both absolute error and percentage error between actual and predicted values. \n",
        "    Furthermore, it performs error weighting based on actual values. In seasonal sales time series scenarios, \n",
        "    seasonal peak periods are assigned a higher weight, so errors in these periods have a more significant impact on this metric.\n",
        "    \n",
        "    Weighting is performed by assigning higher weights to larger actual values, since errors in high demand periods \n",
        "    can have a more relevant impact on planning decisions and financial results.\n",
        "    \n",
        "    In the code section where weight is defined as 'weight = actual', it is considered that the weight of each observation \n",
        "    is equal to the actual value of that observation itself. This means that when calculating WAPE, absolute percentage errors \n",
        "    are multiplied by the actual values themselves before being summed to calculate total weighted error.\n",
        "    \"\"\"\n",
        "    \n",
        "    # Initializes variables to store total WAPE and total weights\n",
        "    total_wape = 0\n",
        "    total_weight = 0\n",
        "    \n",
        "    try:\n",
        "        # Loop over actual (y) and predicted (yhat) values\n",
        "        for i in range(len(y)):\n",
        "            actual = y[i]  # Actual value\n",
        "            predicted = yhat[i]  # Predicted value\n",
        "            \n",
        "            # Calculates absolute error between actual and predicted value\n",
        "            absolute_error = abs(actual - predicted)\n",
        "            \n",
        "            # Calculates absolute percentage error\n",
        "            absolute_percentage_error = absolute_error / actual\n",
        "            \n",
        "            # Calculates weight as the actual value (used for weighting)\n",
        "            weight = actual\n",
        "            \n",
        "            # Updates total WAPE summing weighted percentage error\n",
        "            total_wape += absolute_percentage_error * weight\n",
        "            \n",
        "            # Updates total weights\n",
        "            total_weight += weight\n",
        "        \n",
        "        # Calculates final WAPE as weighted average of percentage errors\n",
        "        wape = total_wape / total_weight * 100\n",
        "    except:\n",
        "        # If an exception occurs (e.g., division by zero), sets WAPE as infinite\n",
        "        wape = np.inf\n",
        "        \n",
        "    return wape\n",
        "\n",
        "\n",
        "# Function to perform predictions and calculate metrics\n",
        "def predict_and_evaluate(model, serie_tgrt, serie_tgrt_test, series_cov, scale_trgt, transformer_trgt, target_col):\n",
        "    df_preds = pd.DataFrame(columns=['NET_VALUE_predicted_05', 'NET_VALUE_predicted', 'NET_VALUE_predicted_95', 'NET_VALUE_real'])\n",
        "    metrics_lists = {\n",
        "        'mape': [], 'smape': [], 'ope': [], 'r2': [], 'rmse': [], 'wape': []\n",
        "    }\n",
        "\n",
        "    unique_dates = serie_tgrt_test.time_index.strftime('%Y-%m-%d').unique()\n",
        "    min_time = serie_tgrt.time_index.min()\n",
        "\n",
        "    progress_bar_general = tqdm(unique_dates, desc='Processing')\n",
        "    for day in progress_bar_general:\n",
        "        start_time = pd.Timestamp(day)\n",
        "        start_real = pd.Timestamp(day) - pd.Timedelta(days=1)\n",
        "        end_time = pd.Timestamp(day)\n",
        "\n",
        "        serie_tgrt_filt = serie_tgrt.slice(min_time, start_real)\n",
        "        serie_tgrt_test_filt = serie_tgrt_test.slice(start_time, end_time)\n",
        "        current_series = serie_tgrt_filt\n",
        "        predictions_05, predictions_median, predictions_95 = [], [], []\n",
        "\n",
        "        if model.uses_past_covariates or model.uses_future_covariates:\n",
        "\n",
        "            # Predicts\n",
        "            pred = model.predict(\n",
        "                series=current_series,\n",
        "                past_covariates=series_cov,\n",
        "                future_covariates=series_cov[future_variables],\n",
        "                n=1,\n",
        "                predict_likelihood_parameters=True)\n",
        "            predictions_05.append(pred[f'{target_col}_q0.05'].values().item())\n",
        "            predictions_median.append(pred[f'{target_col}_q0.50'].values().item())\n",
        "            predictions_95.append(pred[f'{target_col}_q0.95'].values().item())\n",
        "            current_series = current_series.append(pred[f'{target_col}_q0.50'])\n",
        "        else:\n",
        "            pred = model.predict(series=serie_tgrt_filt, n=len(serie_tgrt_test_filt))\n",
        "            predictions_median = pred.values().flatten()\n",
        "\n",
        "        pred_05_series = TimeSeries.from_times_and_values(serie_tgrt_test_filt.time_index, np.array(predictions_05))\n",
        "        pred_series = TimeSeries.from_times_and_values(serie_tgrt_test_filt.time_index, np.array(predictions_median))\n",
        "        pred_95_series = TimeSeries.from_times_and_values(serie_tgrt_test_filt.time_index, np.array(predictions_95))\n",
        "\n",
        "        if scale_trgt:\n",
        "            pred_05_series = transformer_trgt.inverse_transform(pred_05_series)\n",
        "            pred_series = transformer_trgt.inverse_transform(pred_series)\n",
        "            pred_95_series = transformer_trgt.inverse_transform(pred_95_series)\n",
        "            serie_tgrt_test_filt = transformer_trgt.inverse_transform(serie_tgrt_test_filt)\n",
        "\n",
        "        pred_05_series = pred_05_series.map(replace_negatives_with_zero)\n",
        "        pred_series = pred_series.map(replace_negatives_with_zero)\n",
        "        pred_95_series = pred_95_series.map(replace_negatives_with_zero)\n",
        "\n",
        "        if start_time.weekday() < 5:\n",
        "            df_trgt = pred_series.pd_dataframe()\n",
        "            if df_trgt.index.weekday.unique() < 5:\n",
        "                mape = metrics.mape(serie_tgrt_test_filt + 1, pred_series + 1)\n",
        "                smape = metrics.smape(serie_tgrt_test_filt + 1, pred_series + 1)\n",
        "                ope = metrics.ope(serie_tgrt_test_filt + 1, pred_series + 1)\n",
        "                r2 = metrics.r2_score(serie_tgrt_test_filt + 1, pred_series + 1)\n",
        "                rmse = metrics.rmse(serie_tgrt_test_filt + 1, pred_series + 1)\n",
        "                wape = calculate_wape(serie_tgrt_test_filt + 1, pred_series + 1)\n",
        "\n",
        "                metrics_lists['mape'].append(mape)\n",
        "                metrics_lists['smape'].append(smape)\n",
        "                metrics_lists['ope'].append(ope)\n",
        "                metrics_lists['r2'].append(r2)\n",
        "                metrics_lists['rmse'].append(rmse)\n",
        "                metrics_lists['wape'].append(wape.values().squeeze().item())\n",
        "\n",
        "        df_preds = pd.concat([df_preds, \n",
        "                              pred_05_series.stack(pred_series).stack(pred_95_series).pd_dataframe().rename(\n",
        "                                  columns={'0': 'NET_VALUE_predicted_05', '0_1': 'NET_VALUE_predicted', '0_1_1': 'NET_VALUE_predicted_95'})])\n",
        "\n",
        "    return df_preds, metrics_lists"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n",
        "## 3. Input Data and Registered Model Import"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Acquiring max date already predicted\n",
        "str_select_dt_max_predicted = '''\n",
        "                                SELECT max(dt)\n",
        "                                FROM analytics.refined_sales_orders_forecast\n",
        "                                '''\n",
        "\n",
        "dt_max_predicted = spark.sql(str_select_dt_max_predicted).toPandas()\n",
        "dt_max_predicted = dt_max_predicted.iloc[0].dt.strftime('%Y-%m-%d').values[0]\n",
        "dt_max_predicted"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Sales Series\n",
        "str_select_sales = '''\n",
        "                        SELECT *\n",
        "                        FROM analytics.refined_sales_orders_agg\n",
        "                        '''\n",
        "\n",
        "df_series_sales = spark.sql(str_select_sales).toPandas()\n",
        "\n",
        "# Setting SYSTEM_TIMESTAMP column as DataFrame index\n",
        "df_series_sales.set_index('SYSTEM_TIMESTAMP', inplace=True)\n",
        "df_series_sales.sort_index(inplace=True)\n",
        "\n",
        "# REMOVE THIS LINE! Created only to generate the first data mass\n",
        "#df_series_sales = df_series_sales[df_series_sales.index < dt_max_predicted]\n",
        "\n",
        "# Acquiring last full period\n",
        "last_full_period = df_series_sales.index.max()\n",
        "last_full_period"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "df_series_sales.info()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Importing registered model from mlflow\n",
        "model_stage = 'staging'\n",
        "mlflow_model_name = f'models:/LightGBM_forecast_sales_daily/{model_stage}'\n",
        "model = mlflow.sklearn.load_model(mlflow_model_name)\n",
        "\n",
        "# Loading scaler\n",
        "artifacts_root = '/Workspace/Repos/DataScience/FORECAST_PROJECT/src/artifacts/'\n",
        "with open(f'{artifacts_root}transformer_trgt.pkl', 'rb') as f: # must be in the same folder as this notebook\n",
        "    transformer_trgt = pickle.load(f)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Inspecting model parameters\n",
        "model.model_params"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n",
        "## 4. Data Preparation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Converts granularity to Daily\n",
        "df_series_sales_resampled = df_series_sales.resample('D').sum()\n",
        "\n",
        "# Filtering up to the last full period\n",
        "df_series_sales_resampled = df_series_sales_resampled[df_series_sales_resampled.index < last_full_period]\n",
        "\n",
        "# Decomposing the series for residual outlier verification\n",
        "result = seasonal_decompose(df_series_sales_resampled['NET_VALUE'], model='additive', period=8)\n",
        "trend = result.trend\n",
        "seasonal = result.seasonal\n",
        "residual = result.resid\n",
        "\n",
        "# Calculates residual z-scores\n",
        "z_scores = stats.zscore(residual.dropna())\n",
        "\n",
        "# Gets outlier indices\n",
        "zscore_threshold = 3\n",
        "outliers = np.abs(z_scores) > zscore_threshold\n",
        "outliers_index = residual.dropna().index[outliers]\n",
        "\n",
        "# Residual interpolation at outlier points\n",
        "residual_adjusted = residual.copy()\n",
        "residual_adjusted[outliers_index] = np.nan\n",
        "residual_adjusted = residual_adjusted.interpolate()\n",
        "residual_adjusted = residual_adjusted.fillna(method='bfill').fillna(method='ffill')\n",
        "\n",
        "# Reconstruction of adjusted time series\n",
        "adjusted_time_series = trend + seasonal + residual_adjusted\n",
        "df_series_sales_resampled['NET_VALUE_clean'] = adjusted_time_series\n",
        "df_series_sales_resampled['NET_VALUE_clean'] = df_series_sales_resampled['NET_VALUE_clean'].fillna(df_series_sales_resampled['NET_VALUE'])\n",
        "\n",
        "# Creating calendar variables\n",
        "df_series_sales_resampled['year'] = df_series_sales_resampled.index.year\n",
        "df_series_sales_resampled['month'] = df_series_sales_resampled.index.month\n",
        "df_series_sales_resampled['week_of_year'] = df_series_sales_resampled.index.isocalendar().week\n",
        "df_series_sales_resampled['day_of_week'] = df_series_sales_resampled.index.day_of_week #  Monday=0 and Sunday=6\n",
        "df_series_sales_resampled['day_of_month'] = df_series_sales_resampled.index.day\n",
        "\n",
        "# Calculates zscore\n",
        "df_series_sales_resampled['zscore'] = stats.zscore(df_series_sales_resampled[['NET_VALUE']])\n",
        "\n",
        "# Holidays to consider if dummies are created\n",
        "events_to_consider = None\n",
        "\n",
        "# Numerical variables\n",
        "numeric_features = ['day_of_week', 'day_of_month', 'month']\n",
        "future_variables = ['day_of_week', 'day_of_month', 'month']\n",
        "\n",
        "# Generate day of week dummy\n",
        "dummy_day_of_week = False\n",
        "\n",
        "# Use event binary flag or not\n",
        "use_event_flag = False\n",
        "\n",
        "# Scale target series or not - Marked as False to use the loaded scaler\n",
        "scale_trgt = False\n",
        "\n",
        "# Scale covariate series or not\n",
        "scale_cov = False\n",
        "\n",
        "# Scaling method (minmax or power)\n",
        "scale_method = 'power'\n",
        "\n",
        "# Target series column to predict\n",
        "target_col = 'NET_VALUE'\n",
        "\n",
        "# Generating series\n",
        "return_list = prepare_input(df=df_series_sales_resampled, \n",
        "                            target_col=target_col,\n",
        "                            numeric_features=numeric_features, \n",
        "                            use_event_flag=use_event_flag, \n",
        "                            events_to_consider=events_to_consider,\n",
        "                            scale_trgt=scale_trgt,\n",
        "                            scale_cov=scale_cov,\n",
        "                            scale_method=scale_method,\n",
        "                            dummy_day_of_week=dummy_day_of_week)\n",
        "\n",
        "# Extracting objects from return list\n",
        "serie_tgrt = return_list[0]\n",
        "if use_event_flag or events_to_consider or numeric_features:\n",
        "    series_cov = return_list[1]\n",
        "if scale_trgt:\n",
        "    transformer_trgt = return_list[2]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Scaling input data\n",
        "serie_tgrt = transformer_trgt.transform(serie_tgrt)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Inspecting last points of input series\n",
        "serie_tgrt[-100:].plot()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Defining forecast horizon - 3 hours, 6 30-minute points\n",
        "horizon_predict = 1\n",
        "\n",
        "# Preparing future series\n",
        "df_future_vars = pd.DataFrame(index=pd.date_range(start=serie_tgrt.time_index.max(),\n",
        "                                end=serie_tgrt.time_index.max() + timedelta(days=horizon_predict + 20),\n",
        "                                freq='D'))\n",
        "\n",
        "df_future_vars['day_of_week'] = df_future_vars.index.day_of_week #  Monday=0 and Sunday=6\n",
        "df_future_vars['month'] = df_future_vars.index.month\n",
        "df_future_vars['day_of_month'] = df_future_vars.index.day\n",
        "\n",
        "future_covariates = TimeSeries.from_dataframe(df_future_vars,\n",
        "                                              value_cols=future_variables)\n",
        "\n",
        "future_covariates.time_index"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def calculate_zscore(value, mean, std_dev):\n",
        "    \"\"\"\n",
        "    Calculates z-score of a value given the mean and standard deviation of the dataset.\n",
        "    \n",
        "    Parameters:\n",
        "    value (float): The value to calculate z-score for.\n",
        "    mean (float): The mean of the dataset.\n",
        "    std_dev (float): The standard deviation of the dataset.\n",
        "    \n",
        "    Returns:\n",
        "    float: The z-score of the value.\n",
        "    \"\"\"\n",
        "    if std_dev == 0:\n",
        "        raise ValueError(\"Standard deviation cannot be zero.\")\n",
        "    zscore = (value - mean) / std_dev\n",
        "    return zscore\n",
        "\n",
        "# Extracting parameters for zscore calculation of predicted values\n",
        "# Since zscore is a predictive variable, it must be calculated for predicted steps based on original parameters\n",
        "mean = df_series_sales_resampled.NET_VALUE.mean()\n",
        "std_dev = df_series_sales_resampled.NET_VALUE.std()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# From the last date present in loaded data, predict defined horizon\n",
        "unique_dates = pd.DataFrame(index=pd.date_range(start=serie_tgrt.time_index.max(),\n",
        "                                end=serie_tgrt.time_index.max(),\n",
        "                                freq='D')).index.strftime('%Y-%m-%d').unique()\n",
        "\n",
        "progress_bar_general = tqdm(unique_dates, desc='Processing')\n",
        "i = 0\n",
        "for day in progress_bar_general:\n",
        "    \n",
        "    current_serie_tgrt = serie_tgrt.copy()\n",
        "    current_series_cov = series_cov.copy()\n",
        "    predictions_05, predictions_median, predictions_95 = [], [], []\n",
        "        \n",
        "    # Updates progress bar description for series\n",
        "    progress_bar_general.set_description(f'Processing day {day}')\n",
        "\n",
        "    # Performs point prediction\n",
        "    pred = model.predict(\n",
        "                            series=current_serie_tgrt,\n",
        "                            past_covariates=current_series_cov,\n",
        "                            future_covariates=future_covariates,\n",
        "                            n=1,\n",
        "                            predict_likelihood_parameters=True)\n",
        "                        \n",
        "    # Updating target series with predicted median\n",
        "    current_serie_tgrt = current_serie_tgrt.append(pred[f'{target_col}_q0.50'])\n",
        "\n",
        "    # If first point, preds assumes predicted point. Otherwise, preds receives one more predicted point\n",
        "    if i == 0:\n",
        "        preds = pred.copy()\n",
        "    else:\n",
        "        preds = preds.append(pred)\n",
        "    \n",
        "    # Extracting calendar information\n",
        "    predicted_timestamp = current_serie_tgrt.time_index[-1]\n",
        "    day_of_week = predicted_timestamp.dayofweek\n",
        "    day_of_month = predicted_timestamp.day\n",
        "    month = predicted_timestamp.month        \n",
        "\n",
        "    # Updating covariate series\n",
        "    new_row = pd.DataFrame({\n",
        "                            'day_of_week': [day_of_week],\n",
        "                            'day_of_month': [day_of_month],\n",
        "                            'month': [month]\n",
        "                            }, \n",
        "                            index=[predicted_timestamp])\n",
        "    df_current_series_cov = current_series_cov.pd_dataframe()\n",
        "    df_current_series_cov = pd.concat([df_current_series_cov, new_row])\n",
        "    current_series_cov = TimeSeries.from_dataframe(df_current_series_cov)\n",
        "\n",
        "    i = i+1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Inspecting still scaled prediction\n",
        "preds.plot()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Last predicted point\n",
        "preds.time_index.max()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Creating dataframe with predictions\n",
        "df_preds = preds.pd_dataframe()\n",
        "\n",
        "# Reversing prediction scaling\n",
        "df_preds[f'{target_col}_q0.05'] = transformer_trgt.inverse_transform(TimeSeries.from_dataframe(df_preds[[f'{target_col}_q0.05']])).map(replace_negatives_with_zero).values().squeeze()\n",
        "df_preds[f'{target_col}_q0.50'] = transformer_trgt.inverse_transform(TimeSeries.from_dataframe(df_preds[[f'{target_col}_q0.50']])).map(replace_negatives_with_zero).values().squeeze()\n",
        "df_preds[f'{target_col}_q0.95'] = transformer_trgt.inverse_transform(TimeSeries.from_dataframe(df_preds[[f'{target_col}_q0.95']])).map(replace_negatives_with_zero).values().squeeze()\n",
        "\n",
        "# Adjusting columns\n",
        "df_preds.columns = ['forecast_NET_VALUE_quartile_0_05', 'forecast_NET_VALUE_quartile_0_5', 'forecast_NET_VALUE_quartile_0_95']\n",
        "\n",
        "# Adjusting to integer values\n",
        "df_preds[df_preds.columns.tolist()] = df_preds[df_preds.columns.tolist()].round(0).astype(int)\n",
        "\n",
        "# Including prediction timestamp in D granularity\n",
        "#dt_predict = pd.Timestamp((datetime.datetime.now() - timedelta(hours=3)).strftime('%Y-%m-%d %H:%M:%S'))\n",
        "#dt_predict_start = (dt_predict + pd.Timedelta(minutes=30)).floor('D')\n",
        "df_preds['dt_predict'] = datetime.datetime.now().strftime('%Y-%m-%d %H:%M:%S')\n",
        "df_preds.sort_index(inplace=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Inspecting prediction chart\n",
        "display(df_preds.reset_index())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Inspecting final data\n",
        "df_preds.info()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Saving data for dates not yet predicted\n",
        "if len(df_preds[df_preds.index > dt_max_predicted]) > 0:\n",
        "    df_spark = spark.createDataFrame(df_preds[df_preds.index > dt_max_predicted].reset_index().rename(columns={'SYSTEM_TIMESTAMP': 'dt'})) \n",
        "    mode = 'append' # overwrite or append\n",
        "    overwriteSchema = 'False' # True or False\n",
        "    df_spark.write.option(\"overwriteSchema\", overwriteSchema).saveAsTable('analytics.refined_sales_orders_forecast', \n",
        "                                                                            format='delta', \n",
        "                                                                            mode=mode,\n",
        "                                                                            path='/dbfs/mnt/datalake/datascience/raw/forecast_sales/output/sales_orders_forecast')\n",
        "    print('Prediction registered in table!')\n",
        "else:\n",
        "    print('No new predictions!')"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 4
}
